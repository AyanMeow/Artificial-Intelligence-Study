## 1 词向量   
将单词按照含义进行编码成向量的方式称为word embedding。  
### 1.1 word2vec  
它的核心思想是通过词的上下文得到词的向量化表示，有两种方法：CBOW（通过附近词预测中心词）、Skip-gram（通过中心词预测附近的词）。  
![image](https://pic3.zhimg.com/80/v2-c509de0c808367acf62194cd976bc166_720w.webp)  
Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。词频越高，被删除的概率越大。  
#### 1.1.1 通过附近词预测中心词CBOW   
**CBOW对高频词更友好。从训练角度来看，在更新参数的时候，低频词和高频词一起被更新，它们的更新一方面次数比较少，一方面，每次更新也受到了高频词的影响。**  
通过目标词的上下文的词预测目标词，例如取大小为2的窗口，通过目标词前后两个词预测目标词。常用的窗口大小是5。  
由于CBOW使用的是词袋模型，因此这上下文单词都是平等的，也就是不考虑上下文单词和目标单词之间的距离大小，只要在我们上下文之内即可。  
具体的做法是，设定词向量的维度d，对所有的词随机初始化为一个d维的向量，然后要对上下文所有的词向量编码得到一个隐藏层的向量，通过这个隐藏层的向量预测目标词，CBOW中的做法是简单的相加，然后做一个softmax的分类，例如词汇表中一个有V个不同的词，就是隐藏层d维的向量乘以一个W矩阵（ $R^{d * V}$ ）转化为一个V维的向量，然后做一个softmax的分类。由于V词汇的数量一般是很大的，每次训练都要更新整个W矩阵计算量会很大，同时这是一个样本不均衡的问题，不同的词的出现次数会有很大的差异，**所以论文中采用了两种不同的优化方法多层Softmax和负采样。**  
```python
if (hs) for (d = 0; d < vocab[word].codelen; d++) {
        f = 0;
        l2 = vocab[word].point[d] * layer1_size;
        // Propagate hidden -> output
        for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2];
        if (f <= -MAX_EXP) continue;
        else if (f >= MAX_EXP) continue;
        else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
        // 'g' is the gradient multiplied by the learning rate
        g = (1 - vocab[word].code[d] - f) * alpha;
        // Propagate errors output -> hidden
        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
        // Learn weights hidden -> output
        // Update weights using the derived error ... but what are these weights??
        for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];
      }
// hidden -> in
      for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
          c = sentence_position - window + a;
          if (c < 0) continue;
          if (c >= sentence_length) continue;
          last_word = sen[c];
          if (last_word == -1) continue;
          for (c = 0; c < layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];
        }
```  
#### 1.1.2 通过中心词预测附近的词Skip-gram  
**Skip-gram对低频词更友好，但计算量更大。因为每个词在作为中心词时，都要对上下文词预测一次，都要进行2C次的预测、调整，当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。**  
跟CBOW的原理相似，但结构相反，它的输入是目标词，输出是上下文单词，先是将目标词映射为一个隐藏层向量，根据这个向量预测目标词上下文两个词，因为词汇表大和样本不均衡，同样也会采用多层softmax或负采样优化。
```python
for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
        c = sentence_position - window + a;
        if (c < 0) continue;
        if (c >= sentence_length) continue;
        last_word = sen[c];
        if (last_word == -1) continue;
        l1 = last_word * layer1_size;
        for (c = 0; c < layer1_size; c++) neu1e[c] = 0;
        // HIERARCHICAL SOFTMAX
        if (hs) for (d = 0; d < vocab[word].codelen; d++) {
          f = 0;
          l2 = vocab[word].point[d] * layer1_size;
          // Propagate hidden -> output
          for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];
          if (f <= -MAX_EXP) continue;
          else if (f >= MAX_EXP) continue;
          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
          // 'g' is the gradient multiplied by the learning rate
          g = (1 - vocab[word].code[d] - f) * alpha;
          // Propagate errors output -> hidden
          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
          // Learn weights hidden -> output
          for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];
        }
        for (c = 0; c < layer1_size; c++) syn0[c + l1] += neu1e[c];
```
#### 1.1.3 多层softmax   
通过词频与霍夫曼树建立词汇表，每一个分叉节点即为一个sigmoid，层层递进直到叶子结点，即为多层softmax。Huffman树的叶子节点并不像上面的模型中有一个对应的输出向量，而是所有内部节点具有一个对应的向量。我们要更新的是内部节点的对应向量。

#### 1.1.4 负采样
它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。    
当我们用训练样本 ( input word: "fox"，output word: "quick") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。  
当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。**对于小规模数据集，选择5-20个比较好，对于大规模数据集则可以仅选择2-5个。**  
效果：假设隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新300×6=1800个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。  

## 2 循环神经网络RNN  
### 2.1 简单RNN simple RNN  
**RNN的主要思想就是把隐藏层/输出层的值保存在memory中，参与到新的输入的计算中。**  
![image2](https://pic2.zhimg.com/80/v2-d0c81b01df6bb9954c8b6f0a9992c741_720w.webp)  
![image3](https://pic4.zhimg.com/80/v2-499ba1e8c5468fc734056ff3e77b77b7_720w.webp)  
可以从这张图【A表示神经网络】更清晰地看出，之所以能够有记忆功能，主要是在t-1时刻产生的输出【可以理解为记忆】，作为输入参与了t时刻的计算。所以，如果不展开，那么看起来就像是在循环了。所以叫做循环神经网络。  
针对每个序列的step函数：  
```python
class RNN:
  # ...
  def step(self, x):
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y
```
### 2.2 长 短期记忆网络LSTM  
提出原因：当需要预测词间隔较远时，rnn的记忆会失效，例如名词+which修饰语句+动词。  
LSTM和RNN的整体架构是一样的，但是 中间的模块的结构不同。相比于一般的RNN，LSTM的“循环单元”有四个网络结构，相互影响：  
![image](https://pic1.zhimg.com/80/v2-7ecaa278b23d1305130217f3594260e0_720w.webp)  
![image](https://pic3.zhimg.com/80/v2-22b8cf54b5535aa324a1de94ee7bf852_720w.webp)  
相较于只有tanh结构的rnn，新加入了三个门。为什么叫“门gate”呢？因为像sigmoid的输出，在0~1之间，而注意到sigmoid的输出都是进行点乘。这样，如果sigmoid的输出是0，也就意味着和它相乘的那个输入变成了0，也就像是这个输入不被通过。这里有三个门，分别是 forget gate，input gate和output gate。input gate用于控制有多少的输入可以被加入到cell中；output gate用于控制新的cell中的内容有多少可以被输出；forget gate用于控制旧的cell中的内容有多少可以保存。至于门到底是打开，还是关闭，这就是网络自己学习到的。  
#### 2.2.1 遗忘门  
LSTM的第一步是决定需要从cell状态中扔掉什么样的信息。由“遗忘门（forget gate）”的sigmoid层决定。输入 $h_{t-1}$ 和 $x_t$ ，输出一个0和1之间的数。1代表“完全保留这个值”，而0代表“完全扔掉这个值”。  
比如对于一个基于上文预测最后一个词的语言模型。cell的状态可能包含当前主语的信息，来预测下一个准确的词。而当我们得到一个新的主语的时候，我们会想要遗忘旧的主题的记忆，应用新的主语的信息来预测准确的词。  
![image](https://pic1.zhimg.com/80/v2-0d3abfdd90b647c8ef60388d9f13ba44_720w.webp)  
#### 2.2.2 输入门  
第二步是决定我们需要在cell state里存储什么样的信息。这个问题有两个部分。第一，一个sigmoid层调用“输入门（input gate）”以决定哪些数据是需要更新的。然后，一个tanh层为新的候选值创建一个向量 $\widetilde{C_t}$ ，这些值能够加入state中。下一步，我们要将这两个部分合并以创建对state的更新。比如还是语言模型，可以表示为想要把新的语言主题的信息加入到cell state中，以替代我们要遗忘的旧的记忆信息。   
![image](https://pic3.zhimg.com/80/v2-e4772713b36c71aa82d83e084fb4f8aa_720w.webp)  
#### 2.2.3 更新cell  
在决定需要遗忘和需要加入的记忆之后，就可以更新旧的cell state $C_{t-1}$ 到新的cell state $C_{t}$ 了。在这一步，我们把旧的state $C_{t-1}$ 与 $f_t$ 相乘，遗忘我们先前决定遗忘的东西，然后我们加上 $i_t*\widetilde{C_{t}}$ ，这可以理解为新的记忆信息，当然，这里体现了新的输入对状态值的更新度是有限制的，我们可以把 $i_t$ 当成一个权重。  
![image](https://pic4.zhimg.com/80/v2-72f5604a67d483212ccd4bea993d3aab_720w.webp)  
#### 2.2.4 输出门  
最后，我们需要决定要输出的东西。这个输出基于我们的cell state，但会是一个过滤后的值。首先，我们运行一个sigmoid层，这个也就是输出门（output gate），以决定cell state中的那个部分是我们将要输出的。然后把cell state放进tanh（将数值压到-1和1之间），最后将它与sigmoid门的输出相乘，这样就只输出了我们想要的部分了。  
![image](https://pic3.zhimg.com/80/v2-52a551e2e082e968f098612cde6695ee_720w.webp)  

**相比与simple RNN，由于LSTM中，状态S是通过累加的方式来计算的，这样的话，就不是一直复合函数的形式了，它的的导数也不是乘积的形式。不存在导数一直是小数从而导致梯度消失的问题。**  
  
使用Keras搭建多层LSTM网络还是比较方便的，我们只需要使用Sequential()进行堆叠即可，一般来说LSTM模块的层数越多 **（一般不超过3层，再多训练的时候就比较难收敛）**，对高级别的时间表示的学习能力越强；同时，最后会加一层普通的神经网路层用于输出结果的降维。lstm层间可以插入dropout层来防止过拟合。


### 2.3 GRU  
GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。GRU更简单，效果也差不多。   
它组合了遗忘门和输入门到一个单独的“更新门”中。它也合并了cell state和hidden state，并且做了一些其他的改变。  
![image](https://pic3.zhimg.com/80/v2-f2716bc289734d8b545926b38a224692_720w.webp)  
GRU有两个门，分别是reset gate $r_t$ 和update gate $z_t$ 。  
然后是计算候选隐藏层（candidate hidden layer） $\widetilde{ℎ_t}$ ，这个候选隐藏层 和LSTM中的 $\widetilde{c_t}$ 是类似，可以看成是当前时刻的新信息，其中 ${r_t}$ 用来控制需要 保留多少之前的记忆，比如如果 ${r_t}$ 为0，那么 $\widetilde{ℎ_t}$ 只包含当前词的信息。  
最后 ${z_t}$ 控制需要从前一时刻的隐藏层 ${ℎ_{t-1}}$ 中遗忘多少信息，需要加入多少当前 时刻的隐藏层信息 $\widetilde{ℎ_t}$ ，最后得到 ${ℎ_t}$ ，直接得到最后输出的隐藏层信息， 需要注意这里与LSTM的区别是GRU中没有output gate。  

## 3 文本分类  
### 3.1 特征工程  

　　特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据-->信息”的过程，而分类器是“信息-->知识”的过程。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑。文本特征工程分为文本预处理、特征提取、文本表示三个部分，最终目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。  

#### 1）文本预处理  

文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。**传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。** 而停用词是文本中一些高频的代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。  

#### 2）特征提取  

向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。特征权重主要是经典的TF-IDF方法及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。  

#### 3）文本表示  

文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。词袋模型的示例如下：( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0) 一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高维度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。  

传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础。  

### 3.2 分类器  
#### Bag-of-words模型  
bag-fo-words model，即词袋模型非常简单，在NLP领域里使用也很广泛。假设有这么一句话："John likes to watch movies， Mary likes movies too."。那这句话用JSON格式的词袋模型表示的话就是：  
```json
BoW = {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1};
```  
**词袋模型关注的是词的出现次数，而没有记录词的位置信息。** 所以不同的语句甚至相反含义的语句其词袋可能是一样的，比如"Mary is quicker than John"和"John is quicker than Mary"这两句话，其词袋是一样的，但含义是完全相反的。所以凡是完全基于词袋模型的一些算法一般也存在这样该问题。  
词袋模型常用做文本相似度的检测。文本相似性很有用，譬如，在微博和各大BBS上，每一篇文章/帖子的下面都有一个推荐阅读，那就是根据一定算法计算出来的相似文章；还可以根据相似度进行信息过滤；也可以用它来进行分类。  
文本相似性计算一个常用的方法就是余弦相似度：  
$cosineSimilarity(A,B)=\frac{A·B}{\parallel A \parallel_2 \parallel B \parallel_2}$   
注：余弦距离则是 $cosineDist(A,B)=1-cosineSimilaerity(A,B)$  
**传统做法主要问题的文本表示是高维度高稀疏的，特征表达能力很弱。**  
### 3.3 深度学习表示方法：分布式表示（Distributed Representation）  
基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征 $k^n$ 个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。具体一些应用方法见上面的词向量。

#### fasttext  
**fastText结合了自然语言处理和机器学习中的成功的理念，包括了使用词袋以及n-gram表征语句，还有使用子词(subword)信息，并通过隐藏表征在类别间共享信息。** 另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。Fasttext最大的特点是模型简单，只有一层的隐层以及输出层，因此训练速度非常快，在普通的CPU上可以实现分钟级别的训练，比深度模型的训练要快几个数量级。同时，在多个标准的测试数据集上，Fasttext在文本分类的准确率上，和现有的一些深度学习的方法效果相当或接近。  
![image](https://pic3.zhimg.com/80/v2-6e2aca2b9403a1cd59c20cdeea32d22a_720w.webp)  
fastText模型架构:其中x1,x2,…,xN−1,xN表示一个文本中的n-gram向量，这和前文中提到的cbow相似，cbow用上下文去预测中心词，而此处用全部的n-gram去预测指定类别。把句子中所有的词向量进行平均，然后直接接 softmax 层。  

n-gram是基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为N的窗口滑动操作，最终形成窗口为N的字节片段序列。fastText使用两种ngram；词向量的ngram是分解单词获得子词信息，abc => a, ab, abc, b, bc；字/词ngram是组合单词，a,b,c => a, ab, abc, b, bc ， 这个长度由wordNgrams指定，这也是fastText的优势所在。  
##### n-gram算法  
 n-gram模型，称为N元模型，可用于定义字符串中的距离，也可用于中文的分词；该模型假设第n个词的出现只与前面n-1个词相关，与其他词都不相关，整个语句的概率就是各个词出现概率的乘积；而这些概率，利用语料，统计同时出现相关词的概率次数计算得到；常用的模型是Bi-gram和Tri-gram模型。  
假设一个字符串s由m个词组成，因此我们需要计算出P(w1,w2,⋯,wm)的概率，根据概率论中的链式法则得到如下：  
$P(w1,w2,…,wm) = P(w1)* P(w2|w1)* P(w3|w1,w2|)…P(wm|w1,w2…wm-1)$
直接计算这个概率的难度有点大，根据n-gram的假设，当前词仅与前面几个词相关，即  
$P(wi|w1,w2…wi-1) = P(wi|wi-n+1+1,wi-1)$ ，其中i为某个词的位置，n为定义的相关的前几个词，因此得到如下：  
  当n=1时，即一元模型（Uni-gram）  
  $$P(w1,w2,…,wm)=\overset{m}{\underset{i=1}{\prod}}P(w_i)$$    
  但 n=2时，即二元模型（Bi-gram）  
  $$P(w1,w2,…,wm)=\overset{m}{\underset{i=1}{\prod}}P(w_i|w_{i-1})$$    
  当n=3时，即三元模型（Tri-gram）  
  $$P(w1,w2,…,wm)=\overset{m}{\underset{i=1}{\prod}}P(w_i|w_{i-2}w_{i-1})$$    
  **缺点：在语料训练的时候，存在着数据稀疏的情况，因此需要用平滑的技术手段进行处理。**    
  如何通过bi-gram来判断两个句子哪个更合理：（假设以下所有概率都是已知的）  
  $s1=(s) i want english food(/s)$     
  $s2 =(s) want i english food(/s)$
  计算其概率：  
  $P(s1)=P(i|(s))P(want|i)P(english|want)P(food|english)P((/s)|food)=0.25×0.33×0.0011×0.5×0.68=0.000031$  
  $P(s2)=P(want|(s))P(i|want)P(english|want)P(food|english)P((/s)|food)=0.25* 0.0022* 0.0011* 0.5* 0.68 = 0.00000002057$  
  显然s1更为合理。  
  **最大化概率2-gram分词算法：**  
3.1.1算法描述：  

 1、将带分词的字符串从左到右切分为w1,w2,…,wi；  

  2、计算当前词与所有前驱词的概率  

  3、计算该词的累计概率值，并筛选最大的累计概率则为最好的前驱点；  
 
  4、重复步骤3，直到该字符串结束；  

  5、从wi开始，按照从右到左的顺序，依次将没歌词的最佳前驱词输出，即字符串的分词结束。  
  
  
**一些题目：**    
>基于词典的如FMM、BMM，说明【结婚的和尚未结婚的】不同的分词结果：
>>FMM：正向最大匹配算法，假设字典最大长度为4，则一开始查看子串{结婚的和}，无匹配则减一字符：{结婚的}，假设匹配，则输出分词，下一轮由{和尚未结}开始。  
>>BMM：反向最大匹配算法，首轮查看{未结婚的}，无匹配则{结婚的}，匹配，则下一轮由{的和尚未}开始。  
基于统计的如ngram，能够计算条件概率，在实现中为何要构建前缀字典：
>>计算条件概率如上。n-gram的实现需要构建词频表、前缀词频表与前缀概率表，主要是为了便于计算句子概率。  
>
>未登录词：  
>>没有被收录在分词词表中但必须切分出来的词，包括各类专有名词（人名、地名、机构名等）、新词等等。目前已经广泛使用命名实体识别（NER）来识别出名、地名、机构名等专有名词，较好的解决了专有名词的识别难题，而对于新词，可以利用N-Gram+凝聚度+自由度的方法来较好的解决。https://blog.csdn.net/sophiezjz/article/details/83898528
>
>感知机的局限性：
>>感知机的局限性就在于它只能表示由一条直线分割的空间，即线性空间。感知机将权重和偏置设定为参数，单层感知机只能表示线性空间，而含有激活函数的多层感知机可以表示非线性空间。神经网络实际上就是很多个单层感知机的复杂组合。  
>
>为什么要使用激活函数？sigmoid和tanh的区别：
>>将感知机的线性组合变成非线性，从而便于解决非线性空间问题。sigmoid和tanh的值域不同：（0,1）与（-1,1），导函数值域不同：(0,0.25]和(0,1]。sigmoid在输入处于[-1,1]之间时，函数值变化敏感，**一旦接近或者超出区间就失去敏感性，处于饱和状态，影响神经网络预测的精度值。** tanh的输出和输入能够保持非线性单调上升和下降关系，符合BP网络的梯度求解，容错性好，有界，渐进于0、1，符合人脑神经饱和的规律，但比sigmoid函数延迟了饱和期。   
>
>当激活函数的值域不关于0点对称，会导致梯度下降的速度下降，影响学习效率。  
>
>word embedding相较于onehot的优点：
>>独热向量形成的特征矩阵会非常的稀疏，占用的空间非常的大，同时，由于除了自己的那一维为1之外，其余都为0，无法表达词之间的相关性，而word embedding用一个向量来对一个词进行表示。其具有很强的表达关联特征的能力。  
