## 1 词向量   
将单词按照含义进行编码成向量的方式称为word embedding。  
### 1.1 word2vec  
它的核心思想是通过词的上下文得到词的向量化表示，有两种方法：CBOW（通过附近词预测中心词）、Skip-gram（通过中心词预测附近的词）。  
![image](https://pic3.zhimg.com/80/v2-c509de0c808367acf62194cd976bc166_720w.webp)  
Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。词频越高，被删除的概率越大。  
#### 1.1.1 通过附近词预测中心词CBOW   
**CBOW对高频词更友好。从训练角度来看，在更新参数的时候，低频词和高频词一起被更新，它们的更新一方面次数比较少，一方面，每次更新也受到了高频词的影响。**  
通过目标词的上下文的词预测目标词，例如取大小为2的窗口，通过目标词前后两个词预测目标词。常用的窗口大小是5。  
由于CBOW使用的是词袋模型，因此这上下文单词都是平等的，也就是不考虑上下文单词和目标单词之间的距离大小，只要在我们上下文之内即可。  
具体的做法是，设定词向量的维度d，对所有的词随机初始化为一个d维的向量，然后要对上下文所有的词向量编码得到一个隐藏层的向量，通过这个隐藏层的向量预测目标词，CBOW中的做法是简单的相加，然后做一个softmax的分类，例如词汇表中一个有V个不同的词，就是隐藏层d维的向量乘以一个W矩阵（ $R^{d * V}$ ）转化为一个V维的向量，然后做一个softmax的分类。由于V词汇的数量一般是很大的，每次训练都要更新整个W矩阵计算量会很大，同时这是一个样本不均衡的问题，不同的词的出现次数会有很大的差异，**所以论文中采用了两种不同的优化方法多层Softmax和负采样。**  
```python
if (hs) for (d = 0; d < vocab[word].codelen; d++) {
        f = 0;
        l2 = vocab[word].point[d] * layer1_size;
        // Propagate hidden -> output
        for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2];
        if (f <= -MAX_EXP) continue;
        else if (f >= MAX_EXP) continue;
        else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
        // 'g' is the gradient multiplied by the learning rate
        g = (1 - vocab[word].code[d] - f) * alpha;
        // Propagate errors output -> hidden
        for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
        // Learn weights hidden -> output
        // Update weights using the derived error ... but what are these weights??
        for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];
      }
// hidden -> in
      for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
          c = sentence_position - window + a;
          if (c < 0) continue;
          if (c >= sentence_length) continue;
          last_word = sen[c];
          if (last_word == -1) continue;
          for (c = 0; c < layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];
        }
```  
#### 1.1.2 通过中心词预测附近的词Skip-gram  
**Skip-gram对低频词更友好，但计算量更大。因为每个词在作为中心词时，都要对上下文词预测一次，都要进行2C次的预测、调整，当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。**  
跟CBOW的原理相似，但结构相反，它的输入是目标词，输出是上下文单词，先是将目标词映射为一个隐藏层向量，根据这个向量预测目标词上下文两个词，因为词汇表大和样本不均衡，同样也会采用多层softmax或负采样优化。
```python
for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
        c = sentence_position - window + a;
        if (c < 0) continue;
        if (c >= sentence_length) continue;
        last_word = sen[c];
        if (last_word == -1) continue;
        l1 = last_word * layer1_size;
        for (c = 0; c < layer1_size; c++) neu1e[c] = 0;
        // HIERARCHICAL SOFTMAX
        if (hs) for (d = 0; d < vocab[word].codelen; d++) {
          f = 0;
          l2 = vocab[word].point[d] * layer1_size;
          // Propagate hidden -> output
          for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];
          if (f <= -MAX_EXP) continue;
          else if (f >= MAX_EXP) continue;
          else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
          // 'g' is the gradient multiplied by the learning rate
          g = (1 - vocab[word].code[d] - f) * alpha;
          // Propagate errors output -> hidden
          for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
          // Learn weights hidden -> output
          for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];
        }
        for (c = 0; c < layer1_size; c++) syn0[c + l1] += neu1e[c];
```
#### 1.1.3 多层softmax   
通过词频与霍夫曼树建立词汇表，每一个分叉节点即为一个sigmoid，层层递进直到叶子结点，即为多层softmax。Huffman树的叶子节点并不像上面的模型中有一个对应的输出向量，而是所有内部节点具有一个对应的向量。我们要更新的是内部节点的对应向量。

#### 1.1.4 负采样
它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。    
当我们用训练样本 ( input word: "fox"，output word: "quick") 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。  
当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。**对于小规模数据集，选择5-20个比较好，对于大规模数据集则可以仅选择2-5个。**  
效果：假设隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新300×6=1800个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。  

## 2 循环神经网络RNN  
### 2.1 简单RNN simple RNN  
**RNN的主要思想就是把隐藏层/输出层的值保存在memory中，参与到新的输入的计算中。**  
![image](https://pic2.zhimg.com/80/v2-d0c81b01df6bb9954c8b6f0a9992c741_720w.webp)  
![image](https://pic4.zhimg.com/80/v2-499ba1e8c5468fc734056ff3e77b77b7_720w.webp)  
可以从这张图【A表示神经网络】更清晰地看出，之所以能够有记忆功能，主要是在t-1时刻产生的输出【可以理解为记忆】，作为输入参与了t时刻的计算。所以，如果不展开，那么看起来就像是在循环了。所以叫做循环神经网络。  
针对每个序列的step函数：  
```python
class RNN:
  # ...
  def step(self, x):
    # update the hidden state
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    # compute the output vector
    y = np.dot(self.W_hy, self.h)
    return y
```
### 2.2 长 短期记忆网络LSTM  
提出原因：当需要预测词间隔较远时，rnn的记忆会失效，例如名词+which修饰语句+动词。  
LSTM和RNN的整体架构是一样的，但是 中间的模块的结构不同。相比于一般的RNN，LSTM的“循环单元”有四个网络结构，相互影响：  
![image](https://pic1.zhimg.com/80/v2-7ecaa278b23d1305130217f3594260e0_720w.webp)  
![image](https://pic3.zhimg.com/80/v2-22b8cf54b5535aa324a1de94ee7bf852_720w.webp)  
相较于只有tanh结构的rnn，新加入了三个门。为什么叫“门gate”呢？因为像sigmoid的输出，在0~1之间，而注意到sigmoid的输出都是进行点乘。这样，如果sigmoid的输出是0，也就意味着和它相乘的那个输入变成了0，也就像是这个输入不被通过。这里有三个门，分别是 forget gate，input gate和output gate。input gate用于控制有多少的输入可以被加入到cell中；output gate用于控制新的cell中的内容有多少可以被输出；forget gate用于控制旧的cell中的内容有多少可以保存。至于门到底是打开，还是关闭，这就是网络自己学习到的。  
#### 2.2.1 遗忘门  
LSTM的第一步是决定需要从cell状态中扔掉什么样的信息。由“遗忘门（forget gate）”的sigmoid层决定。输入 $h_{t-1}$ 和 $x_t$ ，输出一个0和1之间的数。1代表“完全保留这个值”，而0代表“完全扔掉这个值”。  
比如对于一个基于上文预测最后一个词的语言模型。cell的状态可能包含当前主语的信息，来预测下一个准确的词。而当我们得到一个新的主语的时候，我们会想要遗忘旧的主题的记忆，应用新的主语的信息来预测准确的词。  
![image](https://pic1.zhimg.com/80/v2-0d3abfdd90b647c8ef60388d9f13ba44_720w.webp)  
#### 2.2.2 输入门  
第二步是决定我们需要在cell state里存储什么样的信息。这个问题有两个部分。第一，一个sigmoid层调用“输入门（input gate）”以决定哪些数据是需要更新的。然后，一个tanh层为新的候选值创建一个向量 $\overset{~}{C_t}$ ，这些值能够加入state中。下一步，我们要将这两个部分合并以创建对state的更新。比如还是语言模型，可以表示为想要把新的语言主题的信息加入到cell state中，以替代我们要遗忘的旧的记忆信息。   
![image](https://pic3.zhimg.com/80/v2-e4772713b36c71aa82d83e084fb4f8aa_720w.webp)  
#### 2.2.3 更新cell  
在决定需要遗忘和需要加入的记忆之后，就可以更新旧的cell state $C_{t-1}$ 到新的cell state $C_{t}$ 了。在这一步，我们把旧的state $C_{t-1}$ 与 $f_t$ 相乘，遗忘我们先前决定遗忘的东西，然后我们加上 $i_t*\overset{\~}{C_{t}}$ ，这可以理解为新的记忆信息，当然，这里体现了新的输入对状态值的更新度是有限制的，我们可以把 $i_t$ 当成一个权重。  
![image](https://pic4.zhimg.com/80/v2-72f5604a67d483212ccd4bea993d3aab_720w.webp)  
#### 2.2.4 输出门  
最后，我们需要决定要输出的东西。这个输出基于我们的cell state，但会是一个过滤后的值。首先，我们运行一个sigmoid层，这个也就是输出门（output gate），以决定cell state中的那个部分是我们将要输出的。然后把cell state放进tanh（将数值压到-1和1之间），最后将它与sigmoid门的输出相乘，这样就只输出了我们想要的部分了。  
![image](https://pic3.zhimg.com/80/v2-52a551e2e082e968f098612cde6695ee_720w.webp)  

**相比与simple RNN，由于LSTM中，状态S是通过累加的方式来计算的，这样的话，就不是一直复合函数的形式了，它的的导数也不是乘积的形式。不存在导数一直是小数从而导致梯度消失的问题。**  


### 2.3 GRU  
GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。GRU更简单，效果也差不多。   
它组合了遗忘门和输入门到一个单独的“更新门”中。它也合并了cell state和hidden state，并且做了一些其他的改变。  
![image](https://pic3.zhimg.com/80/v2-f2716bc289734d8b545926b38a224692_720w.webp)  
GRU有两个门，分别是reset gate $r_t$ 和update gate $z_t$ 。  
然后是计算候选隐藏层（candidate hidden layer） $\ovserset{\~}{ℎ_t}$ ，这个候选隐藏层 和LSTM中的 $\ovserset{\~}{c_t}$ 是类似，可以看成是当前时刻的新信息，其中 ${r_t}$ 用来控制需要 保留多少之前的记忆，比如如果 ${r_t}$ 为0，那么 $\ovserset{\~}{ℎ_t}$ 只包含当前词的信息。  
最后 ${z_t}$ 控制需要从前一时刻的隐藏层 ${ℎ_{t-1}}$ 中遗忘多少信息，需要加入多少当前 时刻的隐藏层信息 $\ovserset{\~}{ℎ_t}$ ，最后得到 ${ℎ_t}$ ，直接得到最后输出的隐藏层信息， 需要注意这里与LSTM的区别是GRU中没有output gate。  
