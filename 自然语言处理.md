## 1 词向量   
将单词按照含义进行编码成向量的方式称为word embedding。  
### 1.1 word2vec  
它的核心思想是通过词的上下文得到词的向量化表示，有两种方法：CBOW（通过附近词预测中心词）、Skip-gram（通过中心词预测附近的词）。  
![image](https://pic3.zhimg.com/80/v2-c509de0c808367acf62194cd976bc166_720w.webp)  
#### 1.1.1 通过附近词预测中心词CBOW   
通过目标词的上下文的词预测目标词，图中就是取大小为2的窗口，通过目标词前后两个词预测目标词。  
具体的做法是，设定词向量的维度d，对所有的词随机初始化为一个d维的向量，然后要对上下文所有的词向量编码得到一个隐藏层的向量，通过这个隐藏层的向量预测目标词，CBOW中的做法是简单的相加，然后做一个softmax的分类，例如词汇表中一个有V个不同的词，就是隐藏层d维的向量乘以一个W矩阵（ $R^{d * V}$ ）转化为一个V维的向量，然后做一个softmax的分类。由于V词汇的数量一般是很大的，每次训练都要更新整个W矩阵计算量会很大，同时这是一个样本不均衡的问题，不同的词的出现次数会有很大的差异，**所以论文中采用了两种不同的优化方法多层Softmax和负采样。**  

#### 1.1.2 通过中心词预测附近的词Skip-gram  
跟CBOW的原理相似，它的输入是目标词，先是将目标词映射为一个隐藏层向量，根据这个向量预测目标词上下文两个词，因为词汇表大和样本不均衡，同样也会采用多层softmax或负采样优化。
