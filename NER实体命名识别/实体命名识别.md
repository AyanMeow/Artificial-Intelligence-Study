### 实体分类

3大类：实体类、时间类、数字类

7小类：人名、地名、组织机构；时间、日期、货币量、百分数（正则匹配解决）

### 基本思想

（中文分词）——词实体标注——单个实体识别——复合实体识别

### 标注方法

IOB：I内部，O外部，B开始，B/I-XXX

BIOES：B开始，I内部，E结束，O外部，S单独形成实体

IOB-1：IOB与BIO字母对应的含义相同，其不同点是IOB中，标签B**仅用于**两个连续的同类型命名实体的边界区分，不用于命名实体的起始位置。

### 方法

#### 基于规则

依赖词典、模板、正则表达式

比如，XX喜欢吃东西。XX一定是人名。

#### 基于统计

![1698715842698](image/中文命名识别/1698715842698.png)

选择概率最大的：CRF条件随机场

常用方法统计：

![1698716015838](image/中文命名识别/1698716015838.png)

##### 马尔科夫

###### 马尔科夫性

xt只依赖于xt-1，不依赖于t-2既之前的过去。具有这样性质的随机序列被称为马尔科夫链。

###### 隐马尔可夫模型

假设：1。观测独立性，观测概率只受当前状态影响；2。马尔科夫性假设

![1698716711773](image/中文命名识别/1698716711773.png)

![1698717074042](image/中文命名识别/1698717074042.png)

五要素：1.隐状态节点集；2.观测集；3.状态转移矩阵；4.观测概率矩阵；5.π，第一个状态节点时隐状态节点集中的某一个的概率分布。

首先，隐状态节点 it 是不能直接观测到的数据节点，ot 才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系， it 在A的指导下生成下一个隐状态节点 it+1 ，并且it 在 B 的指导下生成依赖于该 it的观测节点ot , 并且我只能观测到序列 （o1,⋯,ot) 。

根据概率图分类，可以看到HMM属于有向图，并且是生成式模型。并且不难看出HMM是1-gram依赖，超过1-gram的依赖无法被考虑，限制了HMM的性能。

**训练过程：（常用）极大似然估计+隐状态序列：** 算A，算B，估计π

![1698717674762](image/中文命名识别/1698717674762.png)

**训练过程2：前向后向：**不存在隐状态，实际上是EM过程，迭代收敛往复。由于不存在隐状态序列，因此需要给出ABπ的初值。

**序列标注过程：**对已知sample建模HMM模型，后续新sample都可以直接使用，就是在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个。

**序列概率过程：**已知Sa和Sb的HMM模型，对Sc判断属于Sa还是Sb，只要分别过一遍HMM模型，选出概率最大的那个。

###### MEMM

MEMM因为是判别模型，所以与HMM模型有着较大的区别，直接为了确定边界而去建模，比如说序列求概率（分类）问题，直接考虑找出函数分类边界。

![1698718439093](image/中文命名识别/1698718439093.png)

MEMM当前隐藏状态 it 应该是依赖当前时刻的观测节点ot 和上一时刻的隐藏节点it−1

![1698718794157](image/中文命名识别/1698718794157.png)

###### 层叠隐马尔可夫

这个模型的主要优点有两个：

（1）能够对自然语言处理领域的不同抽象层次和长度范围进行更好地建模；

（2）能够通过层次化结构对观测序列中距离较长时段的观察变量进行关联关系推断。

![1698721267299](image/中文命名识别/1698721267299.png)

上图是截取了HHMM的一部分。实际上，HHMM必须是一个满多叉树，即每个叶子节点到树根的距离是一样的。解决了HMM模型的1-gram限制。即可以理解为，叶子结点是一个字母级别的HMM，上一层则是2个字母级别的HMM，等等。

父节点的子节点之间才能跳转，不能跨越父节点进行同层次的跳转。因此，每个父节点下的子节点的最后一个会被标记为end；

###### Viterbi(维特比)算法

维特比算法是一种动态规划算法用于最可能产生观测时间序列的-维特比路径-隐含状态序列，特别是在马尔可夫信息源上下文和隐马尔科夫模型中。换句话说，在以概率为cost的路径问题中，运用动态规划选择cost最大的路径。

#### 基于深度学习

基本思想：全部表示（重点表示）+选择概率最大的实体。

1. HMM
2. Bi-LSTM + CRF
3. BERT+CRF
4. BERT+GlobalPointer
5. NEZHA+CRF
6. NEZHA+GlobalPointer

##### GlobalPointer

具体来说，假设要识别文本序列长度为**n**，简单起见先假定只有一种实体要识别，并且假定每个待识别实体是该序列的一个连续片段，长度不限，并且可以相互嵌套（两个实体之间有交集），那么该序列有多少个“候选实体”呢？不难得出，答案是**n**(**n**+**1**)**/**2个，即长度为**n**的序列有**n**(**n**+**1**)**/**2个不同的连续子序列，这些子序列包含了所有可能的实体，而我们要做的就是从这**n**(**n**+**1**)**/**2“候选实体”里边挑出真正的实体，其实就是一个“**n**(**n**+**1**)**/**2选**k**”的多标签分类问题。如果有**m**种实体类型需要识别，那么就做成**m**个“**n**(**n**+**1**)**/**2选**k**”的多标签分类问题。这就是GlobalPointer的基本思想

![1698721848075](image/中文命名识别/1698721848075.png)

设长度为**n**的输入**t**经过编码后得到向量序列 **[**h**1**,**h**2,**⋯**,**h**n]，通过变换$q_{i,\alpha}=W_{q,\alpha}h_i+b_{q,\alpha}$**和$q_{i,\alpha}=W_{q,\alpha}h_i+b_{q,\alpha}$**我们可以得到序列向量序列**[**q**1**,**α**,**q**2,**α**,**⋯**,**q**n,**α**]和**[**k**1,**α**,**k**2,**α**,**⋯**,**k**n,**α**],它们是识别第**α**种类型实体所用的向量序列。此时我们可以定义
**$S_{\alpha}(i,j)=q^T_{i,\alpha}k_{j,\alpha}$**

作为从**i**到**j**的连续片段是一个类型为**α**的实体的打分。也就是说，用**q**i,**α与**k**j**,**α的内积，作为片段**t[**i**:**j**]是类型为**α**的实体的打分（logits），这里的**t**[**i**:**j**]指的是序列**t**的第**i**个到第**j**个元素组成的连续子串。在这样的设计下，GlobalPointer事实上就是Multi-Head Attention的一个简化版而已，有多少种实体就对应多少个head，相比Multi-Head Attention去掉了**V**相关的运算。

实际上训练语料比较有限的情况下，它的表现往往欠佳，因为它没有显式地包含相对位置信息。所以加入旋转位置编码。

$$
S_{\alpha}(i,j)=q^T_{i,\alpha}\mathcal{R}_{i-j}k_{j,\alpha}
$$

损失函数：softmax+交叉熵

单标签多分类：

![1698723082306](image/中文命名识别/1698723082306.png)log(sum(exp))近似于max函数。这个loss的特点是，所有的非目标类得分{**s**1,**⋯**,**s**t−**1**,**s**t+**1**,**⋯**,**s**n}**跟目标类得分**{**s**t}两两作差比较，它们的差的最大值都要尽可能小于零，所以实现了“目标类得分都大于每个非目标类的得分”的效果。合理将目标st推广至一个多标签集合，就有多标签多分类：

![1698723170270](image/中文命名识别/1698723170270.png)

其中缩放因子**γ**和间隔**m。一般不考虑。**

![1698723346403](image/中文命名识别/1698723346403.png)

##### NEZHA

NEZHA是华为在预训练模型上的实践总结，它在BERT的基础上加了很多当下有用的优化，比如Functional Relative Positional Encoding、Whole Word Masking策略、混合精度训练和Lamb优化器。

###### Functional Relative Positional Encoding

Transformer和BERT都使用到了位置编码，一个是函数式位置编码，即使用正余弦函数来对位置进行编码；另一个是参数式位置编码，即把位置嵌入理解成模型参数的一部分，通过预训练来学习。它们都是绝对位置编码。后来《Self-attention with relative position representations》提出一个参数式的相对位置编码，将相对位置信息加到了Transformer的自注意力层；Transformer-XL和XLNet提出了函数式相对位置编码。所以本文主要的内容就是采用函数式相对位置编码。

![1698735981700](image/实体命名识别/1698735981700.png)

###### Whole Word Masking

BERT-WWM的意思就是在对汉字进行掩码时，会直接对整个词语进行遮掩，所以需要使用分词工具。NEZHA使用jieba作为中文分词工具。

**Mixed Precision Training**

混合精度可以提高训练效率2-3倍，同时也减少了模型的占用空间，这样便可以使用较大的batch size

##### ID-CNN

计算速度快但效果不如Bi-LSTM

相较于传统的CNN，IDCNN能够捕捉更长的上下文信息，同时，相比于LSTM，IDCNN能够实现并行计算。

正常CNN的filter，都是作用在输入矩阵一片连续的位置上，不断sliding做卷积，接着通过pooling来整合多尺度的上下文信息，这种方式会损失分辨率。既然网络中加入pooling层会损失信息，降低精度。那么不加pooling层会使感受野变小，学不到全局的特征。如果我们单纯的去掉pooling层、扩大卷积核的话，这样纯粹的扩大卷积核势必导致计算量的增大，此时最好的办法就是Dilated Convolutions（扩张卷积或叫空洞卷积）。

###### Dilated Convolutions

![1698739027031](image/实体命名识别/1698739027031.png)

* a是普通的卷积过程(dilation rate = 1),卷积后的感受野为3
* b是dilation rate = 2的空洞卷积,卷积后的感受野为5
* c是dilation rate = 3的空洞卷积,卷积后的感受野为8

另外,**空洞卷积可以增大感受野,但是可以不改变图像输出特征图的尺寸(分辨率,resolution)**

![1698739244306](image/实体命名识别/1698739244306.png)

###### ID-CNN结构

在IDCNN中，通过重复使用自膨胀卷积的模块，来达到共享参数的目的以防止过拟合。对于IDCNN的输出也可以使用维特比算法来进行序列标签的预测。

![1698739695985](image/实体命名识别/1698739695985.png)

```
nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=kernel_size, padding=kernel_size // 2, dilation=(1,))
```

torch直接增加dilation选项就行。

###### 蒸馏方法

使用知识蒸馏，student模型用IDCNN模型（参数量约250万 膨胀间隙 1 1 2 4），teacher模型使用Bert类模型，直接用BERT模型在大量无标注数据上进行标注，将其one-hot标签作为训练数据喂给student模型。最终IDCNN模型的CPU推理速度是Bert 12层的50倍，Bert 6层的30倍，且真实场景效果相差1个点以内。

##### Lattice-LSTM

着重于中文NER任务，中文NER和分词相关。直观来看，中文NER可以先进行分词， 然后应用词序列标注。但是不正确的分词可能导致NER错误。但是基于字符的方法的缺点在于，没有充分利用句子中显式的词和词序信息，这些信息可能对识别效果有潜在的提升作用。

Lattice LSTM模型，能够通过门控单元，将不同的词信息传递到每个字符，这种结构能够从上下文中自动发现更加有用的词，提升NER的效果。比起基于字符和基于词的方法，这种结构能够有效的利用词信息，而且也不会出现分词错误。

![1699179936792](image/实体命名识别/1699179936792.png)

###### 结构

Lattice LSTM可以看做是基于字符的NER模型的扩展，其添加了词作为输入(比如下图输入中包括“南京市”)和额外的门(下图的绿色线连接的门控单元)来控制信息流动。

![1698741277514](image/实体命名识别/1698741277514.png)

![1698741667024](image/实体命名识别/1698741667024.png)

![1698741716616](image/实体命名识别/1698741716616.png)

![1698748083339](image/实体命名识别/1698748083339.png)

###### 缺点：

**计算性能低下，不能batch并行化** 。究其原因主要是每个字符之间的增加word cell（看作节点）数目不一致；

**可迁移性差** ：只适配于LSTM，不具备向其他网络迁移的特性

##### Flat Lattice Transformer

采用全连接的自注意力模型来对序列中的长距离依赖进行建模。为了保留位置信息，Transformer引入了序列中每个token的位置表示。受位置表示概念的启发，我们为lattice结构设计了一种巧妙的位置编码，如图1(c)所示。细节为，我**们为一个token(字符或单词)分配两个位置索引：头部位置和尾部位置，通过它们我们可以从一组token中重建一个lattice。因此，我们可以直接使用Transformer完全建模lattice输入。**

![1699179988193](image/实体命名识别/1699179988193.png)

Flat-lattice结构由不同长度的跨度组成。 为了对跨度之间的交互进行编码，我们提出跨度的相对位置编码。 对于lattice中的两个跨度xi和xj，它们之间存在三种关系：相交，包含和分离，由它们的头和尾确定。 代替直接编码这三种关系，我们使用密集向量来对它们的关系建模。 通过对头部和尾部信息进行连续变换来计算，因此，我们认为这样不仅可以表示两个标签之间的关系，而且还可以表明更详细的信息，例如字符与单词之间的距离。 令head[i]和tail[i]表示跨度xi的头和尾位置。 可以使用四种相对距离来表示xi和xj之间的关系。

![1699180743139](image/实体命名识别/1699180743139.png)

![1699182002603](image/实体命名识别/1699182002603.png)

![1699180748536](image/实体命名识别/1699180748536.png)

原码计算att_score部分按照式11来：

![1699255304000](image/实体命名识别/1699255304000.png)

##### Bert_MRC_GP

MRC 是指给模型一段文本，然后指定一个问题，让模型在文本中找出该问题的答案。 比如在 Nested NER 中，对于 `Example 2`的这句话，可以设置以下的问题：

> Input1: Find an organization such as company, agency and insititution in the context.

模型返回的是实体所在位置（span），如下所示。

> Output2: [0, 2], [4, 6], [13, 15]

接着对于其它实体，可以再问：

> Input2: Find an country...
> Output2: [1, 1], [5, 5], [14, 14], [18, 18], [27, 27]

所以 Nested NER 任务就转换成了多轮次的问答任务（A multi-turn QA task）。


基于 MRC 的数据格式与常规 NER 的数据格式不太一样， 从常规 NER 转换到 MRT 的格式需要一些特殊处理 （在下面的代码实现中将会看到 MRC 格式数据的具体样子）。

简单来说，每一个样本需要转换成如下形式的三元组表示：

![1699321754087](image/实体命名识别/1699321754087.png)

实体的开始和结束位置各使用一个二分器来确定，它们接收token的嵌入表示，然后判断该token是否是实体的开始（结束）位置。

因为存在嵌套实体，因此不能按照先后顺序去匹配得到的开始和结束位置。

所以使用第3个二分类器来判断所选的i_satrt和j_end的匹配概率。

[Nested-NER-based-BERT-and-MRC/tutor.ipynb at master · longyongchao/Nested-NER-based-BERT-and-MRC (github.com)](https://github.com/longyongchao/Nested-NER-based-BERT-and-MRC/blob/master/tutor.ipynb)

## 小样本NER

### EntLM

EntLM该方法核心思想：抛弃模板，把NER作为语言模型任务，实体的位置预测为label word,非实体位置预测为原来的词，该方法速度较快。

![1698767481520](image/实体命名识别/1698767481520.png)

label word是一系列选择的代表实体类型的代表词，这样子一次前向过程就能预测所有实体。但是如何选择这样的代表词成为了新的问题。

一种方案：

1）采用领域数据构造实体词典；

2）基于实体词典和已有的实体识别模型对中文数据(100 000)进行远程监督，构造伪标签数据；

3）采用预训练的语言模型对计算LM的输出，取实体部分概率较高的top3个词；

4）根据伪标签数据和LM的输出结果，计算词频；

由于可能出现在很多类中都出现的高频标签词，因此需要去除冲突；

5）使用均值向量作为类别的原型，选择top6高频词的进行求平均得到均值向量；

### LightNER

带有pointer network(指针网络)的seq2seq体系结构+可学习的prompt网络指导注意力。

![1698801810703](image/实体命名识别/1698801810703.png)

![1698801801092](image/实体命名识别/1698801801092.png)

# UIE-通用信息抽取

IE结构生成可分解为两个原子操作：

1. **定位：** 指示从句子中定位目标信息块，例如事件中的实体和触发词。
2. **关联：** 表示根据所需的关联（例如，实体对之间的关系或事件及其参数之间的角色）连接不同的信息块。

### 结构化抽取语言（SEL）

具体来说，论文设计了一种统一的结构化抽取语言（SEL），它通过**定位-关联结构**对不同的IE结构进行编码。如图2a所示，每个SEL表达式包含三种类型的语义单元：1）**SPOTNAME**表示存在一个特定的信息块，该信息块的SPOTNAME类型存在于源文本中；2） **ASSONAME**表示源文本中存在特定的信息块，该信息块与ASSONAME关联到其结构中的上层定位信息；3） **INFOSPAN**表示与源文本中特定的定位或关联信息块相对应的文本范围。此外，SEL中的“：”表示从InfoSpan到其定位或关联名称的映射，两个结构指示器“（”和“）”用于在提取的信息之间形成层次结构。

使用SEL，图2b显示了如何表示实体、关系和事件结构。有三个实体，每个实体都表示为一个定位结构，如“person：Steve”、“organization：Apple”和“time：1997”；一种关系，表示为“Steve”和“Apple”之间的关联结构，关联名称为work for；还有一个事件被表示为关联结构，其中触发器是一个定位结构“start-position:became”，其参数与触发器相关：史蒂夫作为雇员，苹果作为雇主，1997年作为时间。

我们可以看到，SEL的优点是：1）统一编码不同的IE结构，因此不同的IE任务可以建模为相同的文本到结构生成过程；2） 高效地表示同一结构中句子的所有提取结果，可以自然地进行联合提取；3） 生成的输出结构非常紧凑，大大降低了解码的复杂度。

![1698830017659](image/实体命名识别/1698830017659.png)

### 结构模式指导器（SSI）

基于SEL，论文提出了 **结构模式指导器（SSI）** ，这是一种基于模式的提示（prompt）机制，用于控制需要发现和关联哪些类型的信息。

为了描述任务的提取目标，结构化模式指导器构建了一个基于模式的提示，并在生成过程中将其用作前缀。具体来说，与spotting-association结构相对应，结构模式指导器包含三种类型的标记段：1）SPOTNAME：特定信息提取任务中的目标spotting名称，如NER任务中的“person”；2）ASSONAME：目标关联名称，如关系提取任务中的“work for”；3） 在每个SPOTNAME、ASSONAME和输入文本序列之前添加的特殊符号（[spot]、[asso]、[text]）。SSI中的所有标记（tokens）都连接在一起，并放在原始文本序列之前。如图3所示，UIE的整个输入形式为

![1698830604315](image/实体命名识别/1698830604315.png)

例如，SSI“[spot] person [spot] company [asso] work for [text]”表示从句子中提取关系模式“the person works for the company”的记录。

![1698832549237](image/实体命名识别/1698832549237.png)

### Dpair正负例对比学习

![1698832914624](image/实体命名识别/1698832914624.png)

### Drecord结构化训练

![1698832957000](image/实体命名识别/1698832957000.png)

### Dtext语义训练

![1698832981979](image/实体命名识别/1698832981979.png)

可以缓解 语义的灾难性遗忘。

### 微调

使用**teacher-forcing 交叉熵损失**对UIE模型进行微调

> Teacher Forcing[1]训练方式指的是当我们在训练一个自回归模型时（比如RNN，LSTM，或者Transformer的decoder部分），我们需要将真实的目标序列（比如我们想要翻译的句子）作为自回归模型的输入，以便模型能够收敛的更快更好。虽然TF训练方式简单，但它会导致exposure bias的问题，即在训练阶段模型使用的输入来自于真实数据分布，而在测试阶段模型每一时刻使用的输入来自于模型上一时刻的预测结果，这两个输入分布之间的差异被称作exposure bias。

为了缓解解码过程中**自回归**模型的 **暴露偏差** （Ranzato et al.，2016；Zhang et al.，2020），论文还设计了一种有效微调的 **拒绝机制** 。然后随机将几个具有负SPOTNAME和ASSONAME：（SPOTNAME，[NULL]）和（ASSONAME，[NULL]）的[NULL]单元插入到真实SEL中。

例如，facility是模式提示中的负样例spot，即“Steve became CEO of Apple in 1997”一句中没有facility实体。因此，在模型学习期间将“（facility:[NULL]）的噪声随机注入目标记录。这样，UIE可以通过生成[NULL]token有效地学习拒绝误导性生成。

# W2NER

### W-W任务

W2NER模型，将NER任务转换预测word-word(备注：中文是字-字)的关系类别，它能够统一处理扁平实体、重叠实体和非连续实体三种NER任务。

![1698886939600](image/实体命名识别/1698886939600.png)

![1698886948599](image/实体命名识别/1698886948599.png)

### 模型架构

![1698887004523](image/实体命名识别/1698887004523.png)

1. 输入的sentence经过EncodeLayer（BERT+BiLSTM）得到word_reps，shape为[batch_size, cur_batch_max_sentence_length, lstm_hidden_size]；
2. 将word_reps经过Conditional Layer Normalization（简称CLN）层，得到cln；cln的shape为[batch_size, cur_batch_max_sentence_length, cur_batch_max_sentence_length, lstm_hidden_size]，表示word pair的embedding；
3. 将word pair的distance_embedding、所在三角区域的region_embedding和word_embedding按最后一个维度拼接起来，得到的conv_inputs，shape为[batch_size, cur_batch_max_sentence_length, cur_batch_max_sentence_length, dist_emb_size + type_emb_size + lstm_hidden_size];
4. 将conv_inputs经过卷积层(核为1*1的常规二维卷积 + 核为3*3的多层空洞卷积)，得到conv_outputs，shape为[batch_size, output_height = cur_batch_max_sentence_length, output_width = cur_batch_max_sentence_length, conv_hidden_size * 3]，这里的3表示空洞卷积的层数；
5. 将卷积层的输出conv_outputs经过CoPredictor层(由Biaffine + MLP组成)，得到output，output的shape为[batch_size, cur_batch_max_sentence_length, cur_batch_max_sentence_length, label_num]，label_num表示word-word关系类别的个数；

![1698887726795](image/实体命名识别/1698887726795.png)

# 一些问题

### 快速提升NER性能

**规则+领域词典**

如果1层lstm+crf，这么直接的打开方式导致NER性能达不到业务目标，这一点也不意外（这是万里长征的第一步～）。这时候除了badcase分析，不要忘记一个快速提升的重要手段： **规则+领域词典** 。

* 在垂直领域，一个不断积累、不断完善的实体词典对NER性能的提升是稳健的，基于规则+词典也可以快速应急处理一些badcase；
* 对于通⽤领域，可以多种分词工具和多种句法短语⼯具进行融合来提取候选实体，并结合词典进行NER。

此外，怎么更好地将实体词典融入到NER模型中，也是一个值得探索的问题（如嵌入到图神经网络中提取特征  ）。

## 模型提升

NER是一个重底层的任务，1层lstm足以很好捕捉NER任务中的方向信息和局部特征了。

我们应该集中精力在embedding层下功夫，那就是 **引入丰富的特征** ：比如char、bigram、词典特征、词性特征、elmo等等，还有更多业务相关的特征；在垂直领域，如果可以预训练一个领域相关的字向量&语言模型，那是最好不过的了。

总之， **底层的特征越丰富、差异化越大越好。** 我们需要构造不同视角下的特征。

## 引入词向量的NER

中文NER通常是基于字符进行标注的，这是由于基于词汇标注存在分词误差问题。但词汇边界对于实体边界是很有用的，我们该怎么把蕴藏词汇信息的词向量“恰当”地引入到模型中呢？一种行之有效的方法就是 **信息无损的、引入词汇信息的NER方法** ，我称之为**词汇增强**。

近年来，基于词汇增强的中文NER主要分为2条主线：

1. **Dynamic Architecture** ：设计一个动态框架，能够兼容词汇输入；
2. **Adaptive Embedding**：基于词汇信息，构建自适应Embedding；

如LatticeLSTM、Flat Lattice Transformer等

## 实体过长问题

如果NER任务中某一类实体span比较长（⽐如医疗NER中的⼿术名称是很长的），直接采取CRF解码可能会导致很多连续的实体span断裂。除了加入规则进行修正外，这时候也可尝试引入**指针网络+CRF**构建**多任务学习**解决。

> 指针网络会更容易捕捉较长的span，不过指针网络的收敛是较慢的，可以对CRF和指针网络设置不同学习率，或者设置不同的loss权重。

## **BERT在NER中的作用**

* 在低耗时场景中，BERT可以作为一个“对标竞品”，我们可以采取**轻量化**的多种策略组合去逼近甚至超越BERT的性能；
* 在垂直领域应用BERT时，我们首先确认领域内的语料与BERT原始的预训练语料之间是否存在gap，如果这个gap越大，那么我们就 **不要停止预训练** ：继续在领域内进行预训练，继续在具体任务上进行预训练。
* 在小样本条件下，利用BERT可以更好帮助我们解决低资源问题：比如基于BERT等预训练模型的文本增强技术 ^[[6]](https://zhuanlan.zhihu.com/p/152463745#ref_6)^ ，又比如与主动学习、半监督学习、领域自适应结合（后续详细介绍）。
* 在竞赛任务中，BERT很有用！我们可以选取不同的预训练语⾔模型在底层进行特征拼接。具体地，可以将char、bigram和BERT、XLNet等一起拼接喂入1层lstm+crf中。语⾔模型的差异越⼤，效果越好。如果需要对语言模型finetune，需要设置不同的学习率。

## **冷启动NER**

⾯临的是⼀个冷启动的NER任务，业务问题定义好后，首先要做的就是维护好一个领域词典，而不是急忙去标数据、跑模型；当基于规则+词典的NER系统不能够满足业务需求时，才需要启动人工标注数据、构造机器学习模型。

当然，我们可以采取一些省成本的标注方式，如结合 **领域化的预训练语言模型+主动学习** ，挖掘那些“不确定性高”、并且“具备代表性”的高价值样本。

## **低资源NER**

即小样本NER。

* 可以使用BERT（领域预训练的BERT）进行 **数据蒸馏** （半监督学习+置信度选择），同时利用实体词典辅助标注。
* 还可以利用 **实体词典+BERT相结合** ，进行**半监督自训练。**

## 多类型实体

例如在医疗领域，实体类型众多，我们往往需要构建一套**多层级、多粒度、多策略**的NER系统。 例如：

* 多层级的NER系统更加“透明”，可以回溯实体的来源（利于医学实体消歧），方便“可插拔”地迭代优化；同时也不需要构建数目众多的实体类型，让模型“吃不消”。
* 多粒度的NER系统可以提高准召。如，第⼀步抽取⽐较粗粒度的实体，通过**模型+规则+词典**等多策略保证⾼召回；第⼆步进⾏细粒度的实体分类，通过模型+规则保证⾼准确。

## NER噪声

一种简单地有效的方式就是对训练集进行交叉验证，然后人工去清洗这些“脏数据”。当然也可以将noisy label learning应用于NER任务，惩罚那些噪音大的样本loss权重。
