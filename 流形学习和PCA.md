# 流形学习

## 1.概念

#### 一句话总结：流形学习用于将高维数据降维映射到低维欧式空间。

 流形（manifold）是几何中的一个概念，它是高维空间中的几何结构，即空间中的点构成的集合。可以简单的将流形理解成二维空间的曲线，三维空间的曲面在更高维空间的推广。下图是三维空间中的一个流形，这是一个卷曲面：
![image](https://pic3.zhimg.com/80/v2-7dcc5b2aa752a5af8f508202c6862f52_720w.webp)
可以很简单地将流形学习理解为将这个卷曲面展开，铺平在二维表面，同时保持了这个面上数据的相对位置关系。
流形学习（manifold learning）假设数据在高维空间的分布位于某一更低维的流形上，基于这个假设来进行数据的分析。对于降维，要保证降维之后的数据同样满足与高维空间流形有关的几何约束关系。除此之外，流形学习还可以用实现聚类，分类以及回归算法。

假设有一个N维空间中的流形M，即M为N维欧氏空间的一个真子集，流形学习降维算法要实现的是M到n维空间的映射，其中n<<N。即将N维空间中流形M上的点映射为n维空间中的点。下面介绍几种典型的流形降维算法，包括局部线性映射，拉普拉斯特征映射，局部保持投影，等距映射。

## 2.局部线性嵌入（LLE）

#### LLE首先假设数据在较小的局部是线性的，也就是说，某一个数据可以由它邻域中的几个样本来线性表示，要注意它依旧是是非线性降维算法。

##### 现有库：from sklearn.manifold._locally_linear import locally_linear_embedding

主要流程如下：
1.计算样本数据间距离：欧式距离，余弦距离等；
2.计算k邻近，其中k是输入，为每个样本可以拥有的邻居数量；
这一步在lle中的假设为xi可以由它的邻居们xij线性表示，并且在降维之后，这些邻居依旧可以表示它，也就是它们的权重几乎不变：
![image](https://img-blog.csdnimg.cn/20201106160725390.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyOTAyOTk3,size_16,color_FFFFFF,t_70#pic_center)
3.初始化权重矩阵，权重系数应有归一化限制；
4.找到 xi与k个近邻之间的线性关系，这显然是个回归问题，通过矩阵和拉格朗日子乘法来求解这个最优化问题；
5.将原样本映射到低维空间，本质其实是取输入数据M的最小的d个非零特征值对应的特征向量，其中d是需要降维到的维度；

lle相对于isomap的计算量要小了很多，但是不同k值的选择可能对结果影响较大：
![image](https://pic4.zhimg.com/80/v2-e4a28aec12e3fe274e735682c225ec6f_720w.webp)

## 2.拉普拉斯映射（LE）

其实和谱聚类的前面的部分差不多，因此谱聚类也可以理解为先用LE对原始高维数据进行降维，对于低维的谱嵌入再进行k-means聚类。
计算流程如下：
1.求解距离矩阵d；
2.求解邻接矩阵W，k邻近法或全连接法；
3.由邻接矩阵求解度矩阵，计算无向图中每个节点的度D；
4.求解拉普拉斯矩阵，L=D-W；
5.对L进行规范化，（a）L=D(1/2)LD(1/2)，（b）L=D(-1)L；
6.进行特征值分解，并选择最小的d个非零特征值对应的特征向量，得到LE嵌入图；

## 3.多维标度法（MDS）

#### 其主要思想是构造低维空间的内积矩阵，使得该内积矩阵中所表达的任意两点之间的距离与高维空间的相应两点距离相等，然后通过对该内积矩阵进行正交特征值分解，析出两个矩阵相乘(即矩阵与矩阵的转置进行相乘)的形式，获得最终的变换矩阵。

具体流程：
![image](https://img-blog.csdnimg.cn/20190304154038671.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70)

## 4.等度量映射（Isomap）

特点是采用测底线距离，即绕曲面表面距离，但是它的计算复杂度很高很高。

# 流形学习方法与PCA对比

LLE：非线性降维PCA：线性降维，得到包含最大差异性的主成分方向KPCA：非线性降维·诸如瑞士卷形空间，PCA难以找到一个合适的超平面去投影，效果会很差。但是KPCA可以去将这些数据映射到更高维的线性可分空间，进而可以用PCA处理这些非线性数据。pca的形象解释：https://zhuanlan.zhihu.com/p/60664507

> pca白化：
> 白化的目的就是降低输入的冗余性；更正式的说，我们希望通过白化过程使得学习算法的输入具有如下性质：(i)特征之间相关性较低；(ii)所有特征具有相同的方差。
> 要实现以上目的，利用pca实现该目的则：在求得协方差的特征值与特征向量以后，用特征向量矩阵的转置左乘原始数据矩阵以实现对数据的旋转变换，再对变换后数据矩阵每一维除以对应标准差(特征值为对应的方差)。

##### 个人认为：LLE尝试保持局部范围内样本间的关系，而PCA则尝试从样本差异最大的方向去映射高维数据，因此可以理解为LLE着重于学习样本间的相似性，而PCA着重于学习它们之间的相异性来达到划分数据的目的。




# PCA

主成分分析（Principal Component Analysis, PCA）是一种线性降维算法，也是一种常用的数据预处理（Pre-Processing）方法。它的目标是是用方差（Variance）来衡量数据的差异性，并将差异性较大的高维数据投影到低维空间中进行表示。绝大多数情况下，我们希望获得两个主成分因子：分别是从数据差异性最大和次大的方向提取出来的，称为PC1(Principal Component 1) 和 PC2（Principal Component 2）。


## **(1) 基于特征值分解协方差矩阵实现PCA算法**

输入：数据集 𝑋={𝑥1,𝑥2,𝑥3,...,𝑥𝑛} ，需要降到k维。

1) 去平均值(即去中心化)，即每一位特征减去各自的平均值。
2) 计算协方差矩阵 1𝑛𝑋𝑋𝑇,注：这里除或不除样本数量n或n-1,其实对求出的特征向量没有影响。
3) 用特征值分解方法求协方差矩阵1𝑛𝑋𝑋𝑇 的特征值与特征向量。
4) 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P。
5) 将数据转换到k个特征向量构建的新空间中，即Y=PX。
