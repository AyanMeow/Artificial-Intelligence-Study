# 概览

[LLM+Embedding构建问答系统的局限性及优化方案 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/641132245)

[大模型+知识库/数据库问答的若干问题（三） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/642125832)

![1701414783188](image/指南/1701414783188.png)

# 一、意图识别

对于长文本，直接将query向量与文本切片向量进行相似度搜索效率低下，精准度低下。且浪费token。

## 1 方式一：关键词/主题词提取 + 基于多个关键词向量搜索

![1701414886716](image/指南/1701414886716.png)

无监督关键词提取方法主要有三类：

**基于统计特征的关键词提取** （TF,TF-IDF）；

* 基于统计特征的关键词提取算法的思想是利用文档中词语的统计信息抽取文档的关键词：**噪音很大**

**基于词图模型的关键词提取** (PageRank,TextRank)；

* 基于词图模型的关键词提取首先要构建文档的语言网络图，然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，这些短语就是文档的关键词；

**基于主题模型的关键词提取** (LDA)

* 基于主题关键词提取算法主要利用的是主题模型中关于主题分布的性质进行关键词提取；

**基于LLM的关键词提取：**

* 优点是对输入没什么限制，缺点是受到LLM能力限制，而且多轮交互效率下降。

> 对知识库使用**关键词向量**代替原始文本向量：
>
> 单个关键词对应一段文本可能太冗余，应该更适合小切片，例如1-2句话的切片，每个切片对应1-2个关键词，降低冗余存取。
>
> 但小切片可能导致上下文信息丢失。

基于多个关键词的搜索（对query提取关键词）可以看作吸取多路召回的思想

![1701427038054](image/指南/1701427038054.png)

从问题的不同角度、层级去进行搜索并获取不同的topk结果。

**关于query的关键词提取+意图补充：**

通过对句子进行 词性标记、成分句法分析，查找

* 主体：并列关系 [张指导, 姚主席]
* 客体：[三分能力]，三分和能力是修饰关系
* 客体还存在三个并列的领事关系（所属关系）：[詹姆斯, 约基奇, 张伯伦]
* 语义关系

随后将主客体和并列的领事关系叉乘平铺后，扩大query关键词。

例子：张指导和姚主席如何评价詹姆斯、约基奇和张柏伦的三分及抢断能力：

词性标注+成分句法分析：

![1701428206835](image/指南/1701428206835.png)

叉乘：

![1701428217971](image/指南/1701428217971.png)

速度上要比LLM快很多，可用于对query的处理。

## 2 方式二：中心化大模型做语义识别

通过深度学习、统计学习，甚至 LLM ，理解用户问题提取语义槽中需要的内容。

例如：通过 System Role 告知LLM 需要提取槽位信息，让 LLM通过多轮对话引导用户给出所有槽位信息。

还是以游戏攻略为例，玩家咨询球员的打法，那么必须提供：球员姓名，年代（比如2020/2022 年），比赛模式。对应的语义槽可以定义为：

```text
"球员打法" : {
        "球员名称" : ____,
        "年代" : ____,
        "比赛模式": ____,
    }
```

通过LLM，还可以完成对用户输入的补充、消歧、引导

* 引导能力，槽位补充能力
  * 用户：我想查一下电量；Bot: 好的，请问您想查的时间范围、空间范围，和分项是？用户：查上个月的整个园区的吧；Bot:好的，那请问是要查照明、空调、动力的还是其他类型的耗电量？用户：就查询各个分项的，和总体的都查询。Bot:好的，那我复述一下，您希望查询，"照明、空调、动力的还是其他类型的耗电量，以及总体耗电量，时间范围是上个月，空间范围是整体园区"，请问是吗？用户：是的/没问题/对。
  * 引导用户表述完整，查询要素，指标、维度、范围
  * 槽位修复能力

    * 比如：“好的，查询目标设置为查询温度，请问查询哪个空间的温度？”
* 指代消除，解析能力
  * 澄清，它、这等指代词的具体含义
* 改写
  * 改写模块其实非常关键，数据库的存储有特定的形式，但是用户不会按照你的底层数据结构去写，例如，用户不见得会输入和平精英，而是吃鸡，数据库里可不见得会存吃鸡。

# 二、Text to SQL

## 1 LLM分解子任务+小模型生成语句

当库表数量繁多，经常需要进行多表联合查询时，LLM处理这个任务时，有两个大的难点：

* 1. LLM当前不具备6张表关联如此复杂的SQL编写能力。
* 2. 受限与Token长度，我们无法直接将所有的表信息提供给模型来生成正确的SQL语句。 基于大模型当前的能力，我们通过系统架构优化的方式，对此问题进行了解决。

![1701738783255](image/指南/1701738783255.png)

会根据Query + Summary的内容让大模型先完成语义提取。 将复杂问题的理解，格式化输出一个固定的ToT的模版内容，将复杂的任务拆解为几个独立的简单步骤。

在此之后，我们将每个小步的任务丢给小模型去处理。 当然在这个环节，我们为了更稳定的效果，还需要跟大模型完成一些交互。例如当小模型输出的格式不准确时，可以使用大模型来对输出进行格式化。

**但是，和大模型服务交互意味着一些内部资源的泄露。只生成TOT模版在一定程度上可以避免库表信息泄露。**

## 2 开源ft

使用开源大模型进行finetune，开源SQL数据集：WikiSQL，Spider，Cspider，Sparc，CHASE，BIRD

## 3 过程

1. 先验过滤：根据用户输入（或TOT模版），通过embedding检索的方式搜索出topk数据表。数据表的 schema 设计非常重要，需要描述清楚这个表它的主体信息以及表中重要字段和字段含义。embedding可以提前存储。
2. 构建Prompt，生成SQL：开头角色和要求的定义+外部知识+库表信息+one/few-shot+问题
3. 检查SQL语句正确性
4. 执行SQL语句，返回格式化结果
5. 根据数据结果，整合、计算，回答问题。

> 思维链不一定完全有益：在模型没有给定数据库值描述和零样本（zero-shot）情况下，模型自身的 COT 推理可以更准确地生成答案。然而，当给定额外的知识（knowledge evidence）后，让 LLM 进行 COT，发现效果并不显著，甚至会下降。因此在这个场景中， LLM 可能会产生知识冲突。如何解决这种冲突，使模型既能接受外部知识，又能从自身强大的多步推理中受益，将是未来重点的研究方向。

# DB-GPT项目

## 数据处理

需要注意5个方面：

**Schema Encoding：** 对数据库的结构模式进行编码。在数据库中，我们通常会通过主键和外键进行跨表连接，通过记录这种连接关系可以保留数据库表列之间的一个结构信息，不至于在直接简单序列化的过程中丢失二维表结构。

**Schema Linking：** 它指的是schema和question单词之间的关系。在一般的对话中，用户他有时并不会非常精准的说出哪一些表哪一些列的名字，这时候就需要模型能够去判断用户在这个问句过程中，到底提到了哪一些表哪一些列。

**Question Dependency Structure：** 由于用户是用自然语言进行询问，因此呢他在句法上存在一些依存关系。词之间不同的依存关系可能指向不同的字段。复杂的句式也可能加大LLM的理解难度。

**Questions Coreference：指代消解。** 这是为多轮的Text-to-SQL任务所设计的一种关系,比如说用户呢在最开始询问which students have a cat at their pet?然后在第二轮用户又问到what are they majoring in?然后在第三轮用户还会问Also, how old are they?第二轮和第三轮中的这个指代词they所指代的呢都是students而不是pets,这样的一个指代关系，也是一个多轮对话中经常出现的一种关系。

**Database Content Mentions：** 最后也是一个非常重要的关系称之为**数据库内容的提及**，因为在实际情况下，用户在询问一个问题时，他可以提到一个具体列中的某个值，那么在这种情况下，信息内容匹配就显得非常重要。比如用户询问Show names of properties that are either (houses)or apartments，由于horses和apartments并不是数据库的一个表明或者一个列名，所以在进行Schema Encoding或者说在序列化的时候并不会被考虑，然而如果只给出这两个单词，模型根本就无法猜测到他们到底是属于哪一个列，所以这里我们会使用模糊匹配的方式，来搜寻可能的数据库内容，并将这种信息告诉模型

综合考虑的格式化信息处理：

```json
{
        "query": sample["query"],#要生成的sql指令
        "question": sample["question"],#输入的问题
        "db_id": db_id,#数据库名称
        "db_path": db_path,#数据库文件的地址
        "db_table_names": schema["table_names_original"],#数据库中的表名
        "db_column_names": [
            {"table_id": table_id, "column_name": column_name}
            for table_id, column_name in schema["column_names_original"]
        ],
        "db_column_types": schema["column_types"],
        "db_primary_keys": [{"column_id": column_id} for column_id in schema["primary_keys"]],
        "db_foreign_keys": [
            {"column_id": column_id, "other_column_id": other_column_id}
            for column_id, other_column_id in schema["foreign_keys"]
        ],
    }
```

一个示例：

```python3
{'query': 'SELECT count(*) FROM head WHERE age  >  56',
   'question': 'How many heads of the departments are older than 56 ?',
   'db_id': 'department_management',
   'db_path': './spider/database',
   'db_table_names': ['department', 'head', 'management'],
   'db_column_names': [{'table_id': -1, 'column_name': '*'},
    {'table_id': 0, 'column_name': 'Department_ID'},
    {'table_id': 0, 'column_name': 'Name'},
    {'table_id': 0, 'column_name': 'Creation'},
    {'table_id': 0, 'column_name': 'Ranking'},
    {'table_id': 0, 'column_name': 'Budget_in_Billions'},
    {'table_id': 0, 'column_name': 'Num_Employees'},
    {'table_id': 1, 'column_name': 'head_ID'},
    {'table_id': 1, 'column_name': 'name'},
    {'table_id': 1, 'column_name': 'born_state'},
    {'table_id': 1, 'column_name': 'age'},
    {'table_id': 2, 'column_name': 'department_ID'},
    {'table_id': 2, 'column_name': 'head_ID'},
    {'table_id': 2, 'column_name': 'temporary_acting'}],
   'db_column_types': ['text',
    'number',
    'text',
    'text',
    'number',
    'number',
    'number',
    'number',
    'text',
    'text',
    'number',
    'number',
    'number',
    'text'],
   'db_primary_keys': [{'column_id': 1}, {'column_id': 7}, {'column_id': 11}],
   'db_foreign_keys': [{'column_id': 12, 'other_column_id': 7},
    {'column_id': 11, 'other_column_id': 1}]}
```

然后从一个数据库中找到与一个问题最匹配的行列数据信息，将该行列的相关数据库的结构（表名、列名、主键、外键等）用自然语言描述出来作为回答。

# 三、结果核验与总结

将本地搜索系统返回的结果进行二次加工，比如发挥 LLM 的：

* 结果检验
* 总结、概括
* 格式整理
* 去重、翻译
* 从会话历史中，提取上下文，进行分析处理等能力

可以使用思维链进行反复推理，来生成最终正确的结果。

# 四、对齐（毒性检验）

LLM 会努力让其回答符合人类的价值观，这一工作在模型训练中叫做“对齐”（Align），让 LLM 拒绝回答仇恨、暴力相关的问题。如果 LLM 未按照设定回答了仇恨、暴力相关问题，我们就称之为检测出了毒性（Toxicity）。

对于业务需求，其毒性的范围实际上增加了，即，所有回答了非公司业务的内容都可以称之为存在毒性。

需要使用 Few-Shot 的方法去构建毒性检测的提示词，让 LLM 在拥有多个示例的情况下，判断用户的提问是否符合企业服务的范围。

生成前检验（问题）+生成后检验（回答）


# 五、一些值得注意的

## 1、数据增强

原生数据不足以进行微调时，尤其是类似问答或选择题这样的情况。

可以利用GPT或其他效果好的大模型生成辅助训练数据；注意类似多选题时，大模型天生会注重顺序问题，所以一个问题可以打乱选项顺序变为多个问题，来减少顺序带来的影响。

## 2、TF-IDF与BM25对比

* TF-IDF

 **tfidf算法的优点** ：简单，快速，如果语料库是不变的话，可以提前离线计算好语料库中所有词的tfidf值。

 **缺点：** 仅以“词频”度量词的重要性，后续构成文档的特征值序列，词之间各自独立，无法反映序列信息；庞大的稀疏矩阵不利于后面处理。

* BM25

bm25 是一种用来评价搜索词和文档之间相关性的算法，用简单的话来描述下bm25算法：我们有一个query和一批文档Ds，现在要计算query和每篇文档D之间的相关性分数。

![1703489811184](image/指南/1703489811184.png)

Q表示Query，qi表示Q分词之后的每个单词；d表示一个文档；Wi表示词qi的权重；R(qi，d)表示词qi与文档d的相关性得分。所以query中每个词与文档的相关性的加权和就是query与文档的相关性得分。而求query中每个词的权重就可以使用上面所讲的tfidf，但常常只用idf。

**bm25其实就是把query中每个词，在每篇文档中的tfidf（改良后的）值求和即为该query与该文档的相似性得分。**

## 3、kv cache

LLM infer性能优化。

当前轮输出token与输入tokens拼接，并作为下一轮的输入tokens。因此第 i+1 轮推理时必然包含了第 i 轮的部分计算。KV Cache的出发点就在这里，缓存当前轮可重复利用的计算结果，下一轮计算时直接读取缓存结果，就是这么简单，不存在什么Cache miss问题。

目前各大模型推理都实现了KV Cache，下面就看如何使用了。我们可以在上面代码基础上修改，主要改动：

* 在推理时新增了 past_key_values 参数，该参数就会以追加方式保存每一轮的K V值。kvcache变量内容为((k,v), (k,v), ..., (k,v))，即有n_layers 个 k,v 组成的一个元组，其中 k 和 v 的维度均为 [b, n_head, s, head_dims]。这里可以顺带计算出每轮推理对应的 cache 数据量为 2∗b∗s∗ℎ∗n_layers，这里 s 值等于当前轮次值。以GPT3-175B为例，假设以 float16 来保存 KV cache，senquence长度为100，batchsize=1，则 KV cache占用显存为 2×100×12288×96×2 Byte= 472MB。
* 推理输出的token直接作为下一轮的输入，不再拼接，因为上文信息已经在 kvcache 中。
