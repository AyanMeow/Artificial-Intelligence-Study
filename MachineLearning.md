# 机器学习相关  
## 1.回归算法   
### 1.1线性回归  
用连续线（平面）去拟合离散数据点。  
#### 似然函数：
对于未知参数θ的关于离散型或连续性随机变量(y|x)，及其观测样本yi|xi在y0....yi处的概率或概率密度。  
#### 残差平方和（SSE）：
预测样本与真实样本差的平方和。   
#### R2指标：
1-（残差平方和/真实样本方差和），R2越接近1说明模型拟合越好。   
#### 梯度下降算法：
针对多参数凸函数的优化问题，找到其“最低点”。即在寻找梯度不断下降直至梯度=0的点。  
1.全体（批量）梯度下降：每次考虑所有样本，速度较慢；   
2.随机梯度下降：迭代速度快，但不稳定，容易陷入局部最优；  
3.小批量梯度下降：每次更新选择一小部分数据，相对较为合适。mini-batch。  
### 1.2逻辑回归  
实际上算是二分类模型。名字来源于logistic分布，存在一个线性决策边界。  
#### 先验概率：
指根据以往经验和分析。在实验或采样前就可以得到的概率。  
#### 后验概率：
指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。   
假设我们现在有两个盒子，分别为红色和蓝色。在红色盒子中放着2个苹果和6个橙子，在蓝色盒子中放着1个橙子和3个苹果。   
绿色表示苹果，橙色代表橙子。假设我们每次实验的时候会随机从某个盒子里挑出一个水果，随机变量B（box）表示挑出的是哪个盒子，并且$P(B=blue) = 0.6$（蓝色盒子被选中的概率），$P(B=red) = 0.4$（红色盒子被选中的概率）。随机变量$F（fruit）$表示挑中的是哪种水果，F的取值为"a (apple)"和"o (orange)"。   
现在假设我们已经得知某次实验中挑出的水果是orange，那么这个orange是从红色盒子里挑出的概率是多大呢？依据贝叶斯公式有：  
$P(B=red|F=o)=\frac{P(F=o|B=red)P(B=red)}{P(F=o)}=\frac{2}{3}$   
易得：
$P(B=blue|F=o)=\frac{1}{3}$   
在上面的计算过程中，我们将P(B=blue)称为先验概率（prior probability），因为我们在得到F是“a”或者“o”之前，就可以得到 
 。同理，将P(B=red|F=o)称为后验概率，因为我们在完整的一次实验之后也就是得到了F的具体取值之后才能得到这个概率。  
 ##### 在逻辑回归任务中常常使用梯度上升求取最大值，于是引入 $J(θ)=-\frac{1}{m}l(θ)$ 转换为梯度下降任务  
 常见的是二分类sigmoid函数或多分类softmax多分类函数。   
 
### 1.3正则化  
#### 1.3.1 L1正则化   
加入了L1范数的的解，即求解 $C||w||_1=|w_1|+|w_2|$ 的最小值的解。其解一定是某个菱形和某条原函数等高线的切点。  
![image](https://pic2.zhimg.com/80/v2-3fef81c912c4ac0fd8e61a007139f855_720w.webp)  
经过观察可以看到，几乎对于很多原函数等高曲线，和某个菱形相交的时候及其容易相交在坐标轴（比如上图），也就是说最终的结果，解的
某些维度及其容易是0，比如上图最终解是 $w=(0,x)$ ，这也就是我们所说的L1更容易得到稀疏解（解向量中0比较多）的原因。
