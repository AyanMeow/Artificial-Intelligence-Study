# LSTM

提出原因：当需要预测词间隔较远时，rnn的记忆会失效，例如名词+which修饰语句+动词。
LSTM和RNN的整体架构是一样的，但是 中间的模块的结构不同。相比于一般的RNN，LSTM的“循环单元”有四个网络结构，相互影响：
![image](https://pic1.zhimg.com/80/v2-7ecaa278b23d1305130217f3594260e0_720w.webp)
![image](https://pic3.zhimg.com/80/v2-22b8cf54b5535aa324a1de94ee7bf852_720w.webp)
相较于只有tanh结构的rnn，新加入了三个门。为什么叫“门gate”呢？因为像sigmoid的输出，在0~1之间，而注意到sigmoid的输出都是进行点乘。这样，如果sigmoid的输出是0，也就意味着和它相乘的那个输入变成了0，也就像是这个输入不被通过。这里有三个门，分别是 forget gate，input gate和output gate。input gate用于控制有多少的输入可以被加入到cell中；output gate用于控制新的cell中的内容有多少可以被输出；forget gate用于控制旧的cell中的内容有多少可以保存。至于门到底是打开，还是关闭，这就是网络自己学习到的。

#### 2.2.1 遗忘门

LSTM的第一步是决定需要从cell状态中扔掉什么样的信息。由“遗忘门（forget gate）”的sigmoid层决定。输入 $h_{t-1}$ 和 $x_t$ ，输出一个0和1之间的数。1代表“完全保留这个值”，而0代表“完全扔掉这个值”。
比如对于一个基于上文预测最后一个词的语言模型。cell的状态可能包含当前主语的信息，来预测下一个准确的词。而当我们得到一个新的主语的时候，我们会想要遗忘旧的主题的记忆，应用新的主语的信息来预测准确的词。
![image](https://pic1.zhimg.com/80/v2-0d3abfdd90b647c8ef60388d9f13ba44_720w.webp)

#### 2.2.2 输入门

第二步是决定我们需要在cell state里存储什么样的信息。这个问题有两个部分。第一，一个sigmoid层调用“输入门（input gate）”以决定哪些数据是需要更新的。然后，一个tanh层为新的候选值创建一个向量 $\widetilde{C_t}$ ，这些值能够加入state中。下一步，我们要将这两个部分合并以创建对state的更新。比如还是语言模型，可以表示为想要把新的语言主题的信息加入到cell state中，以替代我们要遗忘的旧的记忆信息。
![image](https://pic3.zhimg.com/80/v2-e4772713b36c71aa82d83e084fb4f8aa_720w.webp)

#### 2.2.3 更新cell

在决定需要遗忘和需要加入的记忆之后，就可以更新旧的cell state $C_{t-1}$ 到新的cell state $C_{t}$ 了。在这一步，我们把旧的state $C_{t-1}$ 与 $f_t$ 相乘，遗忘我们先前决定遗忘的东西，然后我们加上 $i_t*\widetilde{C_{t}}$ ，这可以理解为新的记忆信息，当然，这里体现了新的输入对状态值的更新度是有限制的，我们可以把 $i_t$ 当成一个权重。
![image](https://pic4.zhimg.com/80/v2-72f5604a67d483212ccd4bea993d3aab_720w.webp)

#### 2.2.4 输出门

最后，我们需要决定要输出的东西。这个输出基于我们的cell state，但会是一个过滤后的值。首先，我们运行一个sigmoid层，这个也就是输出门（output gate），以决定cell state中的那个部分是我们将要输出的。然后把cell state放进tanh（将数值压到-1和1之间），最后将它与sigmoid门的输出相乘，这样就只输出了我们想要的部分了。
![image](https://pic3.zhimg.com/80/v2-52a551e2e082e968f098612cde6695ee_720w.webp)

**相比与simple RNN，由于LSTM中，状态S是通过累加的方式来计算的，这样的话，就不是一直复合函数的形式了，它的的导数也不是乘积的形式。不存在导数一直是小数从而导致梯度消失的问题。**

使用Keras搭建多层LSTM网络还是比较方便的，我们只需要使用Sequential()进行堆叠即可，一般来说LSTM模块的层数越多 **（一般不超过3层，再多训练的时候就比较难收敛）**，对高级别的时间表示的学习能力越强；同时，最后会加一层普通的神经网路层用于输出结果的降维。lstm层间可以插入dropout层来防止过拟合。

# Bi-LSTM

BiLSTM是Bi-directional Long Short-Term Memory的缩写，是由前向LSTM与后向LSTM组合而成。两者在自然语言处理任务中都常被用来建模上下文信息。
例如输入词向量序列为 ${h_1,h_2,h_3}$ ，将该序列与其反向序列 ${h_3,h_2,h_1}$ 分别输入两个LSTM网络，得到$\{[O_{h1},O_{h3}],[O_{h2},O_{h2}],[O_{h3},O_{h1}]\}$，即为 {H_1,H_2,H_3} ，再通过线性层。


### 与lstm对比

BiLSTM（双向长短时记忆网络）相比于LSTM（长短时记忆网络）的优点和缺点如下：

优点：

1. 双向性：BiLSTM可以同时从前向和后向进行计算，可以更好地捕捉序列中的[信息](https://geek.csdn.net/educolumn/4b7516410bb8585d5db30bb2e9a69b47?spm=1055.2569.3001.10083)，提高了模型的准确性。
2. 更准确的预测：BiLSTM可以利用[上下文](https://geek.csdn.net/educolumn/396550aabaa0961ca124e1fe124ebc0f?spm=1055.2569.3001.10083)信息，更准确地预测下一个单词或标签。
3. 更好的特征表示：BiLSTM可以利用[上下文](https://geek.csdn.net/educolumn/396550aabaa0961ca124e1fe124ebc0f?spm=1055.2569.3001.10083)信息，提取更好的特征表示，从而提高模型的鲁棒性和泛化能力。

缺点：

1. 计算量大：由于BiLSTM需要同时进行前向和后向计算，因此计算量比LSTM更大。
2. 参数多：由于BiLSTM需要两个LSTM单元，因此参数数量比LSTM多，需要更多的计算资源和更长的训练[时间](https://geek.csdn.net/educolumn/3fb26f40673739b3a46094a10b56bc3a?spm=1055.2569.3001.10083)。
3. 容易过拟合：由于BiLSTM的参数数量较多，容易过拟合，需要进行正则化等[技术](https://geek.csdn.net/educolumn/4949e93ca0a4345b3197d58843b74d65?spm=1055.2569.3001.10083)来避免过拟合。

# 可变长度输入

填充至最长输入长度。

* torch.nn.utils.rnn.pad_sequence()

0填充。

* torch.nn.utils.rnn.pack_padded_sequence()

它的输出有两部分，分别是 data 和 batch_sizes，第一部分为原来的数据按照 time step 重新排列，而 padding 的部分，直接空过了。batch_sizes则是每次实际读入的数据量，也就是说，RNN **把一个 mini-batch sequence 又重新划分为了很多小的 batch，每个小 batch 为所有 sequence 在当前 time step 对应的值，如果某 sequence 在当前 time step 已经没有值了，那么，就不再读入填充的 0，而是降低 batch_size。**

避免把填充0也纳入计算

* torch.nn.utils.rnn.pad_packed_sequence()

pad_packed_sequence 执行的是 pack_padded_sequence 的逆操作，执行下面的代码，观察输出。

# 过长文本输入

1. 截断：头尾截断，中间截断等
2. 子序列划分：合理地划分子序列，并将不同的子序列分别训练，最终作为原始序列的一部分特征。
3. 与AE结合：可以使用autoencoder将长序列表示成为一个较短的序列，经过训练之后再用decoder解码称为需要的输出形式
4. 随机采样子序列：例如按句号进行划分，随机采样句子重组，一方面也可以数据增强
5. 截断反向传播：只采用后面几个timestep作反向传播
