# LoRA

![1697891089928](image/LoRa/1697891089928.png)

基本原理：**预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果** 。

> 如果一个大模型是将数据映射到高维空间进行处理，这里假定在处理一个细分的小任务时，是不需要那么复杂的大模型的，可能只需要在某个子空间范围内就可以解决，那么也就不需要对全量参数进行优化了，我们可以定义当对某个子空间参数进行优化时，能够达到全量参数优化的性能的一定水平（如90%精度）时，那么这个子空间参数矩阵的秩就可以称为对应当前待解决问题的本征秩（intrinsic rank）。

因此，作者认为参数更新过程中也存在一个‘内在秩’。对于预训练权重矩阵W0，我们可以用一个低秩分解来表示参数更新ΔW，即：

![1697892647469](image/LoRa/1697892647469.png)

训练过程中冻结参数W0，仅训练A和B中的参数。如上图所示，前向传播过程变为：

![1697892705796](image/LoRa/1697892705796.png)

A使用高斯初始化，B使用0初始化。这样能够保证BA为0，在初始阶段时，只有主干分支生效。

**在实际操作中，应当将可微调参数分配到多种类型权重矩阵中，而不应该用更大的秩单独微调某种类型的权重矩阵。**

* **结论1** ：适配更多的权重矩阵（ 𝑊𝑜 , 𝑊𝑘 , 𝑊𝑞 , 𝑊𝑣 ）比适配具有较大秩的单一类型的权重矩阵表现更好。
* **结论2** ：增加秩不一定能够覆盖一个更有意义的子空间，一个低秩的适配矩阵可能已经足够了。

1. LoRA 原理与使用技巧有那些？
   LoRA（Low-Rank Adaptation）是一种参数高效的微调方法，它通过引入低秩分解来减少需要更新的参数数量。LoRA的工作原理是将预训练模型的注意力矩阵或前馈网络矩阵分解为两个低秩矩阵的乘积，其中这两个低秩矩阵被视为可学习的任务特定参数。
   使用LoRA的技巧包括：

   - 适配器初始化：使用预训练模型的参数作为LoRA适配器模块的初始化，以保持模型的稳定性。
   - 低秩分解：选择合适的低秩分解方法，如奇异值分解（SVD）或随机矩阵分解，以实现低秩分解。
   - 逐步学习：逐步调整LoRA适配器模块的参数，避免参数更新的幅度过大。
   - 适配器正则化：使用正则化技术，如权重衰减或dropout，来减少LoRA适配器模块的过拟合风险。
2. LoRA 微调优点是什么？

   - 参数高效：LoRA只更新少量的低秩矩阵，相比全量微调，可以显著减少需要更新的参数数量。
   - 计算效率：由于只更新少量的低秩矩阵，LoRA可以减少计算资源的需求，提高训练和推理的效率。
   - 模型稳定性：LoRA适配器模块可以保持预训练模型的稳定性，减少过拟合风险。
   - 性能提升：LoRA微调可以在不牺牲太多性能的情况下实现参数高效的微调。

## 显存分析：

**主干模型部分：**

首先主干模型的权重都要存储到显存中，这部分显存无法省掉。

其次，虽然只对 LoRA 部分的模型进行优化，但是想要求 LoRA 部分的梯度，那么主干的梯度也是必须要求解出来的，所以主干模型的梯度是必须要求的。

由于不需要优化主干模型，所以主干模型对应的优化器不需要存储，这部分显存可以节省。

**LoRA 模型部分：**

LoRA 模型的权重、梯度、优化器状态都需要存储，这个是没有疑问的。

 **结论** ：LoRA在显存方面就只是节省了主干模型的优化器状态。

另外，在实际使用中，由于主干模型不需要优化，所以这部分可以使用fp16，甚至 int8、int4 量化，也会显得显存消耗大幅减小。

# QLoRA

**分块量化(Block-wise Quantization)**

量化是将输入从存储更多信息的表征映射为存储较少信息的表征的过程。如将FP32的数据转化为INT8，能够节省大量的内存。

全局量化方式存在一个问题，即当输入中存在极大值或者离群值时，一些较小的参数无法被精确的表示，因此量化后的神经网络效果会下降很多。为了缓解这个问题，作者采用了分块量化，即将输入划分为多个block，每个block分别量化。

![1697893635236](image/LoRa/1697893635236.png)

QLORA训练过程跟LORA基本上是一致的。区别在于QLORA模型是按照NF4保存的，训练时需要把参数反量化到bf16后进行训练。

# AdaLORA

LORA的局限性在于其预先规定了每个增量矩阵Δ的秩r必须相同。这样就忽略了不同层、不同类型参数对下游任务的重要程度。

本文需要解决的问题就变成了如何根据下游任务自动的找出重要的参数模块并为其分配更多的可微调参数，以提高模型的微调效果。

AdaLORA主要包含两个模块：

(i)  **SVD形式参数更新（SVD-based adaptation）** ：直接将增量矩阵Δ参数化为SVD的形式，避免了在训练过程中进行SVD计算带来的资源消耗；

> **奇异值分解（SVD** ）

> 奇异值分解 是将任意较复杂的矩阵用更小、更简单的 3个子矩阵的相乘表示 ，用这3个小矩阵来描述大矩 阵重要的特性。

AdaLORA增量矩阵Δ显式的替换为PΛQ，这样既省去到了复杂的SVD计算又能显式的裁剪奇异值。同时，为保证P和Q的正交性，还在训练过程中增加了一个正则化，保证$P^TP=Q^TQ=I$:

![1697893913014](image/LoRa/1697893913014.png)

(ii)  **基于重要程度的参数分配(Importance-aware rank allocation)** :

在模型剪枝中，单个参数的**敏感性**被定义为梯度和权重乘积的绝对值 ，如下式

𝐼(𝑤𝑖𝑗)=|𝑤𝑖𝑗⋅∇𝑤𝑖𝑗𝐿|

其中 𝑤𝑖𝑗 是任意可训练的权重， ∇𝑤𝑖𝑗𝐿 是这个权重的梯度。

在SGD中，这个重要性只是单个batch的样本反应的重要性，我们可以使用滑动平均思想来减轻单个batch样本带来的重要性的评估误差，表示为

𝐼¯(𝑡)(𝑤𝑖𝑗)=𝛽1𝐼¯(𝑡−1)(𝑤𝑖𝑗)+(1−𝛽1)𝐼(𝑡)(𝑤𝑖𝑗)

其中 𝑡 代表的是训练步数， 0<𝛽1<1 是滑动平均中控制历史记录和当前批次占比的超参数。

有了重要性，我们可以计算敏感性的 **不确定性** （Uncertainty） ，它表示的是敏感性的局部时间变化，定义为

 𝑈(𝑡)(𝑤𝑖𝑗)=|𝐼(𝑡)(𝑤𝑖𝑗)−𝐼¯(𝑡)(𝑤𝑖𝑗)| 

对于不确定性，我们最好也对它进行滑动平局，如

𝑈¯(𝑡)(𝑤𝑖𝑗)=𝛽2𝑈¯(𝑡−1)(𝑤𝑖𝑗)+(1−𝛽2)𝑈(𝑡)(𝑤𝑖𝑗)

接下来，我们可以使用敏感性 𝐼¯(𝑡)(𝑤𝑖𝑗) 和不确定性 𝑈¯(𝑡)(𝑤𝑖𝑗) 的乘积来表示这个特征的重要性，如

𝑠(𝑡)(𝑤𝑖𝑗)=𝐼¯(𝑡)(𝑤𝑖𝑗)⋅𝑈¯(𝑡)(𝑤𝑖𝑗)

对于三元组 𝐺𝑘,𝑖 ，它的重要性是三元组三个值的加权和，权值取决于 𝑑1 和 𝑑2 ，如

𝑆𝑘,𝑖=𝑠(𝜆𝑘,𝑖)+1/𝑑1∑𝑗𝑠(𝑃𝑘,𝑗𝑖)+1𝑑2∑𝑗𝑠(𝑄𝑘,𝑖𝑗)

**2.3 如何根据重要性自动计算秩 𝑟 的值？**

这里暂不详述，更新r的方式是将奇异矩阵Λ中不重要的部分置0，从而无需改变矩阵本身的大小。

相比LORA，AdaLORA这种设计方式有两个优点：

* AdaLORA只裁剪奇异值矩阵Λ，并不裁剪奇异向量，因此训练过程中更容易恢复被误删的奇异值。
* AdaLORA的P和Q正交矩阵，而LORA的A和B非正交。AdaLORA训练过程中裁剪操作不会影响其他奇异值对应的奇异向量，因此训练会更稳定泛化性能更好。
