# 机器学习相关  
## 1.回归算法   
### 1.1线性回归  
用连续线（平面）去拟合离散数据点。  
#### 似然函数：
对于未知参数θ的关于离散型或连续性随机变量(y|x)，及其观测样本yi|xi在y0....yi处的概率或概率密度。  
#### 残差平方和（SSE）：
预测样本与真实样本差的平方和。   
#### R2指标：
1-（残差平方和/真实样本方差和），R2越接近1说明模型拟合越好。   
#### 梯度下降算法：
针对多参数凸函数的优化问题，找到其“最低点”。即在寻找梯度不断下降直至梯度=0的点。  
1.全体（批量）梯度下降：每次考虑所有样本，速度较慢；   
2.随机梯度下降：迭代速度快，但不稳定，容易陷入局部最优；  
3.小批量梯度下降：每次更新选择一小部分数据，相对较为合适。mini-batch。  
### 1.2逻辑回归  
实际上算是二分类模型。名字来源于logistic分布，存在一个线性决策边界。  
#### 先验概率：
指根据以往经验和分析。在实验或采样前就可以得到的概率。  
#### 后验概率：
指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。
  
**贝叶斯公式：** $P(A|B)=\frac{P(B|A)P(A)}{P(B)}$  
假设我们现在有两个盒子，分别为红色和蓝色。在红色盒子中放着2个苹果和6个橙子，在蓝色盒子中放着1个橙子和3个苹果。   
绿色表示苹果，橙色代表橙子。假设我们每次实验的时候会随机从某个盒子里挑出一个水果，随机变量B（box）表示挑出的是哪个盒子，并且$P(B=blue) = 0.6$（蓝色盒子被选中的概率），$P(B=red) = 0.4$（红色盒子被选中的概率）。随机变量$F（fruit）$表示挑中的是哪种水果，F的取值为"a (apple)"和"o (orange)"。   
现在假设我们已经得知某次实验中挑出的水果是orange，那么这个orange是从红色盒子里挑出的概率是多大呢？依据贝叶斯公式有：  
$P(B=red|F=o)=\frac{P(F=o|B=red)P(B=red)}{P(F=o)}=\frac{2}{3}$   
易得：
$P(B=blue|F=o)=\frac{1}{3}$   
在上面的计算过程中，我们将P(B=blue)称为先验概率（prior probability），因为我们在得到F是“a”或者“o”之前，就可以得到 
 。同理，将P(B=red|F=o)称为后验概率，因为我们在完整的一次实验之后也就是得到了F的具体取值之后才能得到这个概率。  
 ##### 在逻辑回归任务中常常使用梯度上升求取最大值，于是引入 $J(θ)=-\frac{1}{m}l(θ)$ 转换为梯度下降任务  
 常见的是二分类sigmoid函数或多分类softmax多分类函数。   
 
## 2正则化   
给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。  
### 2.1 L1正则化   
加入了L1范数的的解，即求解 $C||w_1||=|w_1|+|w_2|$ 的最小值的解。其解一定是某个菱形和某条原函数等高线的切点。  
![image](https://pic2.zhimg.com/80/v2-3fef81c912c4ac0fd8e61a007139f855_720w.webp)  
经过观察可以看到，几乎对于很多原函数等高曲线，和某个菱形相交的时候及其容易相交在坐标轴（比如上图），也就是说最终的结果，解的
某些维度及其容易是0，比如上图最终解是 $w=(0,x)$ ，这也就是我们所说的L1更容易得到稀疏解（解向量中0比较多）的原因。   
* L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。  $h=f+C|x|$     

### 2.2 L2正则化   
当加入L2正则化的时候，分析和L1正则化是类似的，也就是说我们仅仅是从菱形变成了圆形而已，同样还是求原曲线和圆形的切点作为最终解。当然与L1范数比，我们这样求的L2范数的从图上来看，不容易交在坐标轴上，但是仍然比较靠近坐标轴。因此这也就是我们老说的，L2范数能让解比较小（靠近0），但是比较平滑（不等于0）。   
* L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。 $h=f+Cx^2$   
  
## 3 范数  
#### 任意范数： $||x||p=\sqrt[p]{\sum_ix_i^p}$  
#### L0范数： $||x||0=\sqrt[0]{\sum_ix_i^0}=j$ ，表示向量 {x} 中非0元素的个数；比如压缩感知 (compressive sensing)，我们很多时候希望最小化向量的 l_0 -范数。 但它是一个NP-hard问题，即直接求解它很复杂、也不可能找到解。    
#### L1范数： $||x||1=\sqrt[1]{\sum_ix_i^1}=\sum_i|x_i|$ ，等于向量中所有元素和，压缩感知模型是将 l_0 -范数最小化问题转换成 l_1 -范数最小化问题。    
#### L2范数： $||x||2=\sqrt[2]{\sum_ix_i^2}$ ，表示向量（或矩阵）的元素平方和开根号  
  
## 4 支持向量机（SVM）    
支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。  
![image](https://pica.zhimg.com/v2-197913c461c1953c30b804b4a7eddfcc_r.webp?source=172ae18b&consumer=ZHI_MENG)  
#### 决策边界（support vector）：
选出来的离两边数据点边界点最远的曲线。  
#### 几何间隔：  
对于给定的数据集 $T$ 和超平面 $w\cdot x+b=0$ ，定义超平面关于样本点  $\left( x_i,y_i \right)$  的几何间隔为  
$\gamma_i=y_i\left(\frac{\boldsymbol{w}}{\lVert \boldsymbol{w} \rVert}\cdot \boldsymbol{x_i}+\frac{b}{\lVert \boldsymbol{w} \rVert} \right)$   
其中 $\gamma=\underset{i=1,2...,N}{\min}\gamma_i$ 就是支持向量到超平面的距离。   
SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。  
经过推导之后，问题  $\underset{w,b}{\max}\gamma$  转化为 $\underset{w,b}{\min}\frac{1}{2}\lVert\boldsymbol{w}\rVert^2$    
这是一个含有不等式约束的凸二次规划问题，可以对其使用拉格朗日乘子法得到其对偶问题（dual problem）。 
#### 核变换：与KPCA想法相同，将非线性可分空间的数据通过核函数映射到高维线性可分空间，再进行分割。  
线性核函数—： $K(x,y)=x^Ty+c$ 其中c是常数。         
常用核函数——高斯核函数： $K(x,y)=\exp(\frac{||x-y||^2}{2\sigma^2})$    
## 5 决策树与集成算法  
是机器学习中的一个经典的监督式学习算法。数据从根结点开始通过一系列判断条件最终落在叶子节点上，完成分类与预测。  
* 决策树由决策结点、分支和叶子结点组成。    
  1.决策结点表示在样本的一个属性上进行的划分；    
  2.分支表示对于决策结点进行划分的输出；  
  3.叶结点代表经过分支到达的类。  
### 5.1 分支处理  
* 往往使用启发式算法来进行决策树的构造，例如，使用贪婪算法对每个结点构造部分最优决策树。    
* 对于一个决策树的构建，最重要的部分就在于其分支处理，即确定在每个决策结点处的分支属性。分支属性的选取即对决策节点上选择哪一个属性来对数据集进行划分，要求每个分支中样本的类别纯度尽可能高，而且不要产生样本数量太少的分支。    
### 5.2 ID3算法  
**ID3算法的核心是在决策树各个结点上应用信息增益准则（不是信息增益率!!!)选择特征，递归地构建决策树。**  
ID3算法是在每个结点处选取能获得最高信息增益的分支属性进行分裂。  
在每个决策结点处划分分支、选取分支属性的目的是将整个决策树的样本纯度提升。  
衡量样本集合纯度的指标则是熵。  
熵的计算： $H(X)=-\sum_ip_i * logp_i ,i=1,2...n$   
计算分支属性对于样本集分类好坏程度的度量——信息增益。  
由于分裂后样本集的纯度提高，则样本集的熵降低，熵降低的值即为该分裂方法的信息增益。  
##### ID3算法优缺点  
**优点：**（理论清晰、方法简单、学习能力较强）  
假设空间包含所有的决策树，搜索空间完整。  
健壮性好，不受噪声影响。  
可以训练缺少属性值的实例。  
**缺点**  
ID3只考虑分类型的特征（ID3算法处理的数据是离散的），没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。  
ID3算法对于缺失值没有进行考虑。  
没有考虑过拟合的问题。  
ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。**信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。（因为考虑到不同的分支结点所包含的样本数不同，给分支结点赋予的权重即样本数越多的分支结点影响就越大。** 所以说，ID3算法会倾向于特征选项较多的特征）划分过程会由于子集规模过小而造成统计特征不充分而停止。  
### 5.3 c4.5算法  
C4.5算法总体思路与ID3类似，都是通过构造决策树进行分类，其区别在于分支的处理，在分支属性的选取上，ID3算法使用信息增益作为度量，而C4.5算法引入了**信息增益率**作为度量。  
##### C4.5算法的优缺点  
**优点**  
产生的分类规则易于理解，准确率较高能处理标称型、连续性数据。  
利用信息增益率选择特征向量，能处理特征缺失的数据。  
构造决策树过程中，进行了剪枝，消除了过度匹配的问题。  
**缺点**  
由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。  
C4.5生成的是多叉树，即一个父节点可以有多个节点。在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。  
C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。  
C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。  
在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。  
### 5.4 CART  
#### CART：使用GINI系数来作为衡量标准。  
GINI系数： $Gini(p)=\sum_kp_k(1-p_k)=1-\sum_kp_k^2,k=1,2....K$  
### 5.5 连续属性离散化   
1.全分支，有多少个属性值就划分多少个分支；  
2.等宽、等频划分，以固定区间划分数据；  
3.聚类划分，先对属性进行聚类，根据簇来进行划分。  
### 5.6 过拟合问题  
决策树很容易过拟合，需要剪枝策略。  
1.预剪枝：提前限制深度、叶子结点个数、叶子结点样本个数等；  
2.后剪枝：通过一定衡量标准来判定：  
REP错误率降低剪枝：从叶子结点向上，依次将决策树的所有子树用其样本中最多的类替换，使用一个测试集进行测试，**记录下对于决策树的每棵子树剪枝前后的误差数之差，选取误差数减少最少的子树进行剪枝，将其用子样本集中最多的类替换 。** 按此步骤自底向上，遍历决策树的所有子树，当发现没有可替换的子树时，即每棵子树剪枝后的误差数都会增多，则剪枝结束。  
PEP悲观剪枝：与REP相比，PEP不再需要构建一个单独的测试集。对误分率增加了一个修正因子。  
CCP代价复杂度剪枝： 定义了代价与复杂度的概念，代价是指在剪枝过程中因为子树被替换而增加的错分样本，复杂度表示剪枝后减少的叶结点数。  

## 6 一些评价标准
 **样本正例，分类正例：真阳TP**   
 **样本正例，分类负例：假阴FN**    
 **样本反例，分类正例：假阳FP**   
 **样本反例，分类反例：真阴TN**   
 **准确率：正反例中分类正确的比率：**  $arruracy=\frac{TP+TN}{TP+FN+FP+TN}$     
 **精确率：正确分类的正例数在分类后正例总数中所占比率：**  $precision=\frac{TP}{TP+FP}$      
 **召回率：正确分类的正例数在正样本总数中所占比率：**  $recall=\frac{TP}{TP+FN}$    
 **F值：精确率和召回率的调和平均值：**  $F=\frac{(\alpha^2+1) * arruracy * recall}{\alpha^2(arruracy+recall)}$   
 **F1值：调和参数 $\alpha$ 取值为1,：**  $F_1=\frac{2 * arruracy * recall}{arruracy+recall}$      
 
 ## 7 集成算法  
 集成学习的基本思想就是将多个分类器组合，从而实现一个预测效果更好的集成分类器。  
 ### 7.1 随机森林算法  
 **随机森林既可以胜任分类任务又可以胜任回归任务。**  
 随机森林采用Bagging的思想，所谓的Bagging就是：  
（1）每次有放回地从训练集中取出 n 个训练样本，组成新的训练集；  
（2）利用新的训练集，训练得到M个子模型；  
（3）**对于分类问题，采用投票的方法，得票最多子模型的分类类别为最终的类别；对于回归问题，采用简单的平均方法得到预测值。**   
随机森林以决策树为基本单元，通过集成大量的决策树，就构成了随机森林。   
**构建决策树：**  
* 第一步：T中共有N个样本，有放回的随机选择N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。  
* 第二步：当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。  
* 第三步：决策树形成过程中每个节点都要按照步骤2来分裂，一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。  
* 第四步：按照步骤1~3建立大量的决策树，这样就构成了随机森林了。  

## 7 EM算法与GMM模型  
### 7.1 EM算法  
**在面对包含隐变量的复杂最大似然函数的参数求解时，会用到EM算法。**  
问题：样本集 ${x(1),....,x(m)}$ 中，包含m个独立的样本。但其中每个样本i对应的类别z（i）是未知的，因此难以使用最大似然求解。  
$l(\theta)=\sum_ip(x_i;\theta)=\sum_ilog\sum_Zp(x_i,z;\theta)$  

